{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF and sklearn\n",
    "\n",
    "In this notebook we will guide you through using TF-IDF with sklearn. First we will define a dataset to work with. The dataset X consists out of 10 strings. The first 5 sentences are about 'Obama' the last 5 about 'lion'. The label of each sentence is stored in y.\n",
    "\n",
    "It is important to check that X and y have the same length, otherwise their might be strings without a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "X = [\"Obama was born on August 4.\",\n",
    "     \"Barack is the only president born outside the contiguous 48 states.\",\n",
    "     \"Barack was born to an American mother of European descent and an African father.\",\n",
    "     'Barack and his mother moved to the University of Washington in Seattle, where they lived for a year.',\n",
    "     'Obama described his struggles as a young adult to reconcile social perceptions of his multiracial heritage.',\n",
    "     'Typically, the lion inhabits grasslands and savannas, but is absent in dense forests.',\n",
    "     'adult male lions have a prominent mane.', \n",
    "     'It is a social species, forming groups called prides.',\n",
    "     'A lion pride consists of a few adult males, related females and cubs.',\n",
    "     'Groups of female lions usually hunt together, preying mostly on large ungulates.']\n",
    "\n",
    "y = ['obama', 'obama', 'obama', 'obama', 'obama', 'lion', 'lion','lion','lion','lion',]\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split the data\n",
    "\n",
    "We want to make a seperate test and training dataset. The training dataset shows the data we will use as a corpus to define a TF-IDF vectorizer. Only the words present in this corpus will be taken into account. The test set will also be transformed to a TF-IDF matrix, but new words will not be taken into account. \n",
    "\n",
    "For this you can use the train_test_split() of sklearn. Please look up how it works. We want to have 25% of the data of X in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 3\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By printing the labels of y_test we see how many sentences with the label 'Obama' or 'lion' are in the test set. Ideally we have at least one of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lion', 'obama', 'lion']\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF vectorizer\n",
    "\n",
    "First we'll define some functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_features(txt, flag):\n",
    "    \"\"\"\n",
    "    Transform the list of strings in txt into a tfidf vector. \n",
    "    If we want to train the tfidf vector we do fit_transform, otherwise we'll do transform\n",
    "    \"\"\"\n",
    "    if flag == \"train\":\n",
    "        x = tfidf.fit_transform(txt)\n",
    "    else:\n",
    "        x = tfidf.transform(txt)\n",
    "    return x \n",
    "\n",
    "\n",
    "def matrix_to_pandas(X, tfidf):\n",
    "    \"\"\"\n",
    "    Transform the sparse tfidf matrix in a pandas dataframe in order to see the column names above the features.\n",
    "    At this stage you don't need to understand what is happening here. Is is only for visualisation purposes.\n",
    "    \"\"\"\n",
    "    X = X.toarray()\n",
    "    df = pd.DataFrame(X, columns = tfidf.get_feature_names())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we define the TfidfVectorizer. please have a close look at the documentation, as there are quite a lot of interesting arguments we can use. We will also demonstrate a couple of them in this notebook.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "Then we train the tfidf matrix. If we print X_tfidf_train.shape we see the dimensions of the matrix. Every row is a new string and every column is a new word. The first number is the number of rows, 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 58)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf_train = tfidf_features(X_train, flag=\"train\")\n",
    "\n",
    "print(X_tfidf_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you now have a look at X_tfidf_train it looks a little weird. You see rows in the following format\n",
    "\n",
    "(0, 6)\t0.515973461359075\n",
    "\n",
    "You have to read it in the following way: for document 0 the 6th word has importance 0.515973461359075\n",
    "\n",
    "It is written this way as there are a lot of zero values in a tfidf matrix and we don't want to waste space on showing them. If you would like to have a look at it in matrix form you can just do:\n",
    "\n",
    "<code>\n",
    "print(X_tfidf_train.toarray())\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t0.515973461359075\n",
      "  (0, 36)\t0.42830228436617496\n",
      "  (0, 8)\t0.42830228436617496\n",
      "  (0, 53)\t0.42830228436617496\n",
      "  (0, 34)\t0.42830228436617496\n",
      "  (1, 39)\t0.3681528517608115\n",
      "  (1, 9)\t0.3681528517608115\n",
      "  (1, 17)\t0.3055984836695788\n",
      "  (1, 16)\t0.3681528517608115\n",
      "  (1, 44)\t0.3681528517608115\n",
      "  (1, 43)\t0.3055984836695788\n",
      "  (1, 23)\t0.3681528517608115\n",
      "  (1, 24)\t0.3681528517608115\n",
      "  (2, 13)\t0.27391477876166354\n",
      "  (2, 1)\t0.27391477876166354\n",
      "  (2, 4)\t0.2273727899808243\n",
      "  (2, 10)\t0.27391477876166354\n",
      "  (2, 12)\t0.27391477876166354\n",
      "  (2, 35)\t0.16873681866084675\n",
      "  (2, 31)\t0.2273727899808243\n",
      "  (2, 2)\t0.27391477876166354\n",
      "  (2, 3)\t0.5478295575233271\n",
      "  (2, 48)\t0.19435072341886617\n",
      "  (2, 7)\t0.2273727899808243\n",
      "  (2, 8)\t0.2273727899808243\n",
      "  :\t:\n",
      "  (4, 43)\t0.22421482608304552\n",
      "  (4, 34)\t0.22421482608304552\n",
      "  (5, 56)\t0.26136237371472965\n",
      "  (5, 15)\t0.26136237371472965\n",
      "  (5, 27)\t0.26136237371472965\n",
      "  (5, 47)\t0.26136237371472965\n",
      "  (5, 55)\t0.26136237371472965\n",
      "  (5, 42)\t0.26136237371472965\n",
      "  (5, 22)\t0.26136237371472965\n",
      "  (5, 54)\t0.26136237371472965\n",
      "  (5, 51)\t0.26136237371472965\n",
      "  (5, 46)\t0.26136237371472965\n",
      "  (5, 32)\t0.26136237371472965\n",
      "  (5, 20)\t0.21695321580014781\n",
      "  (5, 4)\t0.21695321580014781\n",
      "  (5, 35)\t0.1610042935895912\n",
      "  (5, 31)\t0.21695321580014781\n",
      "  (5, 48)\t0.1854444168203422\n",
      "  (5, 7)\t0.21695321580014781\n",
      "  (6, 29)\t0.4312073587067964\n",
      "  (6, 40)\t0.4312073587067964\n",
      "  (6, 18)\t0.4312073587067964\n",
      "  (6, 28)\t0.4312073587067964\n",
      "  (6, 0)\t0.3579391395114768\n",
      "  (6, 26)\t0.3579391395114768\n"
     ]
    }
   ],
   "source": [
    "print(X_tfidf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you execute the following cell you will understand this more clearly. We see the different words that the model has learned in the column names and we see one row per string we have in the training dataset. In the matrix we see the importance of each word for each string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>african</th>\n",
       "      <th>american</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>as</th>\n",
       "      <th>august</th>\n",
       "      <th>barack</th>\n",
       "      <th>born</th>\n",
       "      <th>called</th>\n",
       "      <th>...</th>\n",
       "      <th>to</th>\n",
       "      <th>together</th>\n",
       "      <th>ungulates</th>\n",
       "      <th>university</th>\n",
       "      <th>usually</th>\n",
       "      <th>was</th>\n",
       "      <th>washington</th>\n",
       "      <th>where</th>\n",
       "      <th>year</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.515973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.368153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273915</td>\n",
       "      <td>0.273915</td>\n",
       "      <td>0.54783</td>\n",
       "      <td>0.227373</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227373</td>\n",
       "      <td>0.227373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309394</td>\n",
       "      <td>0.309394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.224215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.27011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.27011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.216953</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261362</td>\n",
       "      <td>0.261362</td>\n",
       "      <td>0.261362</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.357939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      adult   african  american       an       and       as    august  \\\n",
       "0  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000  0.515973   \n",
       "1  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000  0.000000   \n",
       "2  0.000000  0.273915  0.273915  0.54783  0.227373  0.00000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000  0.000000   \n",
       "4  0.224215  0.000000  0.000000  0.00000  0.000000  0.27011  0.000000   \n",
       "5  0.000000  0.000000  0.000000  0.00000  0.216953  0.00000  0.000000   \n",
       "6  0.357939  0.000000  0.000000  0.00000  0.000000  0.00000  0.000000   \n",
       "\n",
       "     barack      born    called  ...        to  together  ungulates  \\\n",
       "0  0.000000  0.428302  0.000000  ...  0.000000  0.000000   0.000000   \n",
       "1  0.000000  0.000000  0.368153  ...  0.000000  0.000000   0.000000   \n",
       "2  0.227373  0.227373  0.000000  ...  0.194351  0.000000   0.000000   \n",
       "3  0.000000  0.000000  0.000000  ...  0.000000  0.309394   0.309394   \n",
       "4  0.000000  0.000000  0.000000  ...  0.191651  0.000000   0.000000   \n",
       "5  0.216953  0.000000  0.000000  ...  0.185444  0.000000   0.000000   \n",
       "6  0.000000  0.000000  0.000000  ...  0.000000  0.000000   0.000000   \n",
       "\n",
       "   university   usually       was  washington     where      year    young  \n",
       "0    0.000000  0.000000  0.428302    0.000000  0.000000  0.000000  0.00000  \n",
       "1    0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  0.00000  \n",
       "2    0.000000  0.000000  0.227373    0.000000  0.000000  0.000000  0.00000  \n",
       "3    0.000000  0.309394  0.000000    0.000000  0.000000  0.000000  0.00000  \n",
       "4    0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  0.27011  \n",
       "5    0.261362  0.000000  0.000000    0.261362  0.261362  0.261362  0.00000  \n",
       "6    0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  0.00000  \n",
       "\n",
       "[7 rows x 58 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_to_pandas(X_tfidf_train, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us experiment with some arguments we could give to the TfidfVectorizer. We will start by adding 'max_features= 15'. We now see only 15 words of the full vocabulary are taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>barack</th>\n",
       "      <th>born</th>\n",
       "      <th>groups</th>\n",
       "      <th>his</th>\n",
       "      <th>lions</th>\n",
       "      <th>mother</th>\n",
       "      <th>obama</th>\n",
       "      <th>of</th>\n",
       "      <th>on</th>\n",
       "      <th>social</th>\n",
       "      <th>to</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693037</td>\n",
       "      <td>0.287640</td>\n",
       "      <td>0.287640</td>\n",
       "      <td>0.28764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.213462</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245865</td>\n",
       "      <td>0.28764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.530690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.530690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393833</td>\n",
       "      <td>0.53069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.347495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.694991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347495</td>\n",
       "      <td>0.257882</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.347495</td>\n",
       "      <td>0.297028</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435138</td>\n",
       "      <td>0.435138</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322923</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.371942</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      adult        an       and    barack     born    groups       his  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.50000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.00000  0.707107  0.000000   \n",
       "2  0.000000  0.693037  0.287640  0.287640  0.28764  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.00000  0.530690  0.000000   \n",
       "4  0.347495  0.000000  0.000000  0.000000  0.00000  0.000000  0.694991   \n",
       "5  0.000000  0.000000  0.435138  0.435138  0.00000  0.000000  0.435138   \n",
       "6  0.707107  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000   \n",
       "\n",
       "      lions    mother     obama        of       on    social        to  \\\n",
       "0  0.000000  0.000000  0.500000  0.000000  0.50000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.00000  0.707107  0.000000   \n",
       "2  0.000000  0.287640  0.000000  0.213462  0.00000  0.000000  0.245865   \n",
       "3  0.530690  0.000000  0.000000  0.393833  0.53069  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.347495  0.257882  0.00000  0.347495  0.297028   \n",
       "5  0.000000  0.435138  0.000000  0.322923  0.00000  0.000000  0.371942   \n",
       "6  0.707107  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000   \n",
       "\n",
       "       was  \n",
       "0  0.50000  \n",
       "1  0.00000  \n",
       "2  0.28764  \n",
       "3  0.00000  \n",
       "4  0.00000  \n",
       "5  0.00000  \n",
       "6  0.00000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features= 15) \n",
    "matrix_to_pandas(tfidf_features(X_train, flag=\"train\"), tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us experiment with some arguments we could give to the TfidfVectorizer. We will add 'stop_words='english'. All stopwords are now removed before the matrix is trained. We see indeed that there are no more stopwords in the columnnames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>barack</th>\n",
       "      <th>born</th>\n",
       "      <th>groups</th>\n",
       "      <th>heritage</th>\n",
       "      <th>lions</th>\n",
       "      <th>mother</th>\n",
       "      <th>multiracial</th>\n",
       "      <th>obama</th>\n",
       "      <th>perceptions</th>\n",
       "      <th>reconcile</th>\n",
       "      <th>social</th>\n",
       "      <th>struggles</th>\n",
       "      <th>university</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.292256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352079</td>\n",
       "      <td>0.292256</td>\n",
       "      <td>0.352079</td>\n",
       "      <td>0.352079</td>\n",
       "      <td>0.292256</td>\n",
       "      <td>0.352079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.648465</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      adult    barack      born    groups  heritage     lions    mother  \\\n",
       "0  0.000000  0.000000  0.707107  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.707107  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.577350  0.577350  0.000000  0.000000  0.000000  0.577350   \n",
       "3  0.000000  0.000000  0.000000  0.707107  0.000000  0.707107  0.000000   \n",
       "4  0.292256  0.000000  0.000000  0.000000  0.352079  0.000000  0.000000   \n",
       "5  0.000000  0.538281  0.000000  0.000000  0.000000  0.000000  0.538281   \n",
       "6  0.707107  0.000000  0.000000  0.000000  0.000000  0.707107  0.000000   \n",
       "\n",
       "   multiracial     obama  perceptions  reconcile    social  struggles  \\\n",
       "0     0.000000  0.707107     0.000000   0.000000  0.000000   0.000000   \n",
       "1     0.000000  0.000000     0.000000   0.000000  0.707107   0.000000   \n",
       "2     0.000000  0.000000     0.000000   0.000000  0.000000   0.000000   \n",
       "3     0.000000  0.000000     0.000000   0.000000  0.000000   0.000000   \n",
       "4     0.352079  0.292256     0.352079   0.352079  0.292256   0.352079   \n",
       "5     0.000000  0.000000     0.000000   0.000000  0.000000   0.000000   \n",
       "6     0.000000  0.000000     0.000000   0.000000  0.000000   0.000000   \n",
       "\n",
       "   university     young  \n",
       "0    0.000000  0.000000  \n",
       "1    0.000000  0.000000  \n",
       "2    0.000000  0.000000  \n",
       "3    0.000000  0.000000  \n",
       "4    0.000000  0.352079  \n",
       "5    0.648465  0.000000  \n",
       "6    0.000000  0.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features= 15, stop_words='english') \n",
    "matrix_to_pandas(tfidf_features(X_train, flag=\"train\"), tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us experiment with some arguments we could give to the TfidfVectorizer. We will add 'ngram_range=(1,3)'. This means that also combination of up until 5 words can be taken into account. An example for which we would like to have this is 'New York'. \n",
    "\n",
    "You see indeed there are now columns that consist of more than 3 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>adult reconcile social</th>\n",
       "      <th>barack</th>\n",
       "      <th>born</th>\n",
       "      <th>groups</th>\n",
       "      <th>lions</th>\n",
       "      <th>mother</th>\n",
       "      <th>multiracial heritage</th>\n",
       "      <th>obama</th>\n",
       "      <th>obama described struggles</th>\n",
       "      <th>reconcile social perceptions</th>\n",
       "      <th>social</th>\n",
       "      <th>social perceptions multiracial</th>\n",
       "      <th>struggles young adult</th>\n",
       "      <th>young adult reconcile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.275669</td>\n",
       "      <td>0.332097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.332097</td>\n",
       "      <td>0.275669</td>\n",
       "      <td>0.332097</td>\n",
       "      <td>0.332097</td>\n",
       "      <td>0.275669</td>\n",
       "      <td>0.332097</td>\n",
       "      <td>0.332097</td>\n",
       "      <td>0.332097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      adult  adult reconcile social    barack      born    groups     lions  \\\n",
       "0  0.000000                0.000000  0.000000  0.707107  0.000000  0.000000   \n",
       "1  0.000000                0.000000  0.000000  0.000000  0.707107  0.000000   \n",
       "2  0.000000                0.000000  0.577350  0.577350  0.000000  0.000000   \n",
       "3  0.000000                0.000000  0.000000  0.000000  0.707107  0.707107   \n",
       "4  0.275669                0.332097  0.000000  0.000000  0.000000  0.000000   \n",
       "5  0.000000                0.000000  0.707107  0.000000  0.000000  0.000000   \n",
       "6  0.707107                0.000000  0.000000  0.000000  0.000000  0.707107   \n",
       "\n",
       "     mother  multiracial heritage     obama  obama described struggles  \\\n",
       "0  0.000000              0.000000  0.707107                   0.000000   \n",
       "1  0.000000              0.000000  0.000000                   0.000000   \n",
       "2  0.577350              0.000000  0.000000                   0.000000   \n",
       "3  0.000000              0.000000  0.000000                   0.000000   \n",
       "4  0.000000              0.332097  0.275669                   0.332097   \n",
       "5  0.707107              0.000000  0.000000                   0.000000   \n",
       "6  0.000000              0.000000  0.000000                   0.000000   \n",
       "\n",
       "   reconcile social perceptions    social  social perceptions multiracial  \\\n",
       "0                      0.000000  0.000000                        0.000000   \n",
       "1                      0.000000  0.707107                        0.000000   \n",
       "2                      0.000000  0.000000                        0.000000   \n",
       "3                      0.000000  0.000000                        0.000000   \n",
       "4                      0.332097  0.275669                        0.332097   \n",
       "5                      0.000000  0.000000                        0.000000   \n",
       "6                      0.000000  0.000000                        0.000000   \n",
       "\n",
       "   struggles young adult  young adult reconcile  \n",
       "0               0.000000               0.000000  \n",
       "1               0.000000               0.000000  \n",
       "2               0.000000               0.000000  \n",
       "3               0.000000               0.000000  \n",
       "4               0.332097               0.332097  \n",
       "5               0.000000               0.000000  \n",
       "6               0.000000               0.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features= 15, stop_words='english', ngram_range=(1,3)) \n",
    "matrix_to_pandas(tfidf_features(X_train, flag=\"train\"), tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Difference between fit_transform() and transform()\n",
    "\n",
    "Let's assume you have a training dataset to start with and you want to train a tfidf model on the corpus within this dataset. You perform a fit_transform() on the dataset.\n",
    "\n",
    "A couple of days later you receive an extra dataset you want to transform (crf test set). Now you only want to transform it, because we want this new dataset to have the same columns and corpus as the training dataset. Let's see what would happen if we use fit_transform again on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>48 states</th>\n",
       "      <th>absent</th>\n",
       "      <th>barack president born</th>\n",
       "      <th>born outside</th>\n",
       "      <th>born outside contiguous</th>\n",
       "      <th>contiguous 48</th>\n",
       "      <th>contiguous 48 states</th>\n",
       "      <th>grasslands</th>\n",
       "      <th>inhabits</th>\n",
       "      <th>lion</th>\n",
       "      <th>outside contiguous</th>\n",
       "      <th>outside contiguous 48</th>\n",
       "      <th>president born outside</th>\n",
       "      <th>savannas</th>\n",
       "      <th>typically</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.322002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.423394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   48 states    absent  barack president born  born outside  \\\n",
       "0   0.000000  0.000000               0.000000      0.000000   \n",
       "1   0.333333  0.000000               0.333333      0.333333   \n",
       "2   0.000000  0.423394               0.000000      0.000000   \n",
       "\n",
       "   born outside contiguous  contiguous 48  contiguous 48 states  grasslands  \\\n",
       "0                 0.000000       0.000000              0.000000    0.000000   \n",
       "1                 0.333333       0.333333              0.333333    0.000000   \n",
       "2                 0.000000       0.000000              0.000000    0.423394   \n",
       "\n",
       "   inhabits      lion  outside contiguous  outside contiguous 48  \\\n",
       "0  0.000000  1.000000            0.000000               0.000000   \n",
       "1  0.000000  0.000000            0.333333               0.333333   \n",
       "2  0.423394  0.322002            0.000000               0.000000   \n",
       "\n",
       "   president born outside  savannas  typically  \n",
       "0                0.000000  0.000000   0.000000  \n",
       "1                0.333333  0.000000   0.000000  \n",
       "2                0.000000  0.423394   0.423394  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_to_pandas(tfidf_features(X_test, flag=\"train\"), tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the columns do not at all correspond with the collumns of the trainingdataset we had earlier. This means there is no way to compare the new transformation with the old transformation. We don't want this. That is why we only transform() the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>adult reconcile social</th>\n",
       "      <th>barack</th>\n",
       "      <th>born</th>\n",
       "      <th>groups</th>\n",
       "      <th>lions</th>\n",
       "      <th>mother</th>\n",
       "      <th>multiracial heritage</th>\n",
       "      <th>obama</th>\n",
       "      <th>obama described struggles</th>\n",
       "      <th>reconcile social perceptions</th>\n",
       "      <th>social</th>\n",
       "      <th>social perceptions multiracial</th>\n",
       "      <th>struggles young adult</th>\n",
       "      <th>young adult reconcile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.275669</td>\n",
       "      <td>0.332097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.332097</td>\n",
       "      <td>0.275669</td>\n",
       "      <td>0.332097</td>\n",
       "      <td>0.332097</td>\n",
       "      <td>0.275669</td>\n",
       "      <td>0.332097</td>\n",
       "      <td>0.332097</td>\n",
       "      <td>0.332097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      adult  adult reconcile social    barack      born    groups     lions  \\\n",
       "0  0.000000                0.000000  0.000000  0.707107  0.000000  0.000000   \n",
       "1  0.000000                0.000000  0.000000  0.000000  0.707107  0.000000   \n",
       "2  0.000000                0.000000  0.577350  0.577350  0.000000  0.000000   \n",
       "3  0.000000                0.000000  0.000000  0.000000  0.707107  0.707107   \n",
       "4  0.275669                0.332097  0.000000  0.000000  0.000000  0.000000   \n",
       "5  0.000000                0.000000  0.707107  0.000000  0.000000  0.000000   \n",
       "6  0.707107                0.000000  0.000000  0.000000  0.000000  0.707107   \n",
       "\n",
       "     mother  multiracial heritage     obama  obama described struggles  \\\n",
       "0  0.000000              0.000000  0.707107                   0.000000   \n",
       "1  0.000000              0.000000  0.000000                   0.000000   \n",
       "2  0.577350              0.000000  0.000000                   0.000000   \n",
       "3  0.000000              0.000000  0.000000                   0.000000   \n",
       "4  0.000000              0.332097  0.275669                   0.332097   \n",
       "5  0.707107              0.000000  0.000000                   0.000000   \n",
       "6  0.000000              0.000000  0.000000                   0.000000   \n",
       "\n",
       "   reconcile social perceptions    social  social perceptions multiracial  \\\n",
       "0                      0.000000  0.000000                        0.000000   \n",
       "1                      0.000000  0.707107                        0.000000   \n",
       "2                      0.000000  0.000000                        0.000000   \n",
       "3                      0.000000  0.000000                        0.000000   \n",
       "4                      0.332097  0.275669                        0.332097   \n",
       "5                      0.000000  0.000000                        0.000000   \n",
       "6                      0.000000  0.000000                        0.000000   \n",
       "\n",
       "   struggles young adult  young adult reconcile  \n",
       "0               0.000000               0.000000  \n",
       "1               0.000000               0.000000  \n",
       "2               0.000000               0.000000  \n",
       "3               0.000000               0.000000  \n",
       "4               0.332097               0.332097  \n",
       "5               0.000000               0.000000  \n",
       "6               0.000000               0.000000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features= 15, stop_words='english', ngram_range=(1,3)) \n",
    "df = matrix_to_pandas(tfidf_features(X_train, flag=\"train\"), tfidf)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>adult reconcile social</th>\n",
       "      <th>barack</th>\n",
       "      <th>born</th>\n",
       "      <th>groups</th>\n",
       "      <th>lions</th>\n",
       "      <th>mother</th>\n",
       "      <th>multiracial heritage</th>\n",
       "      <th>obama</th>\n",
       "      <th>obama described struggles</th>\n",
       "      <th>reconcile social perceptions</th>\n",
       "      <th>social</th>\n",
       "      <th>social perceptions multiracial</th>\n",
       "      <th>struggles young adult</th>\n",
       "      <th>young adult reconcile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   adult  adult reconcile social    barack      born  groups  lions  mother  \\\n",
       "0    1.0                     0.0  0.000000  0.000000     0.0    0.0     0.0   \n",
       "1    0.0                     0.0  0.707107  0.707107     0.0    0.0     0.0   \n",
       "2    0.0                     0.0  0.000000  0.000000     0.0    0.0     0.0   \n",
       "\n",
       "   multiracial heritage  obama  obama described struggles  \\\n",
       "0                   0.0    0.0                        0.0   \n",
       "1                   0.0    0.0                        0.0   \n",
       "2                   0.0    0.0                        0.0   \n",
       "\n",
       "   reconcile social perceptions  social  social perceptions multiracial  \\\n",
       "0                           0.0     0.0                             0.0   \n",
       "1                           0.0     0.0                             0.0   \n",
       "2                           0.0     0.0                             0.0   \n",
       "\n",
       "   struggles young adult  young adult reconcile  \n",
       "0                    0.0                    0.0  \n",
       "1                    0.0                    0.0  \n",
       "2                    0.0                    0.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_to_pandas(tfidf_features(X_test, flag=\"test\"), tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes it worked! The two above matrices now have the same column names. Now we can compare them with eachother!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract important words from each piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have chosen the following document: \n",
      " \n",
      "Obama was born on August 4.\n",
      " \n",
      " \n",
      "The two most important words for this document are list below, next to them you see their score. The lower the score the less relevant.\n",
      " \n",
      "born                      0.707107\n",
      "obama                     0.707107\n",
      "adult                     0.000000\n",
      "adult reconcile social    0.000000\n",
      "barack                    0.000000\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#For which document would you like to see the most important words. Choose a number between 0 and 6\n",
    "doc = 0\n",
    "\n",
    "print('You have chosen the following document: ')\n",
    "print(' ')\n",
    "print(X_train[doc])\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('The two most important words for this document are list below, next to them you see their score. The lower the score the less relevant.')\n",
    "print(' ')\n",
    "print(df.iloc[doc].nlargest(5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
