{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUu8O6h9b3Xr"
   },
   "source": [
    "# Text classification with BERT\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from **Transformers**) is a NLP model developed by Google in 2018. It is a model that is already pre-trained on a 2,5000M (+- 170 GB) words corpus from Wikipedia. \n",
    "\n",
    "![bert](https://www.advisa.fr/wp-content/uploads/2019/10/google-bert-algorithm.jpg)\n",
    "\n",
    "To accomplish a particular NLP task, the pre-trained BERT model is used as a base and refined by adding an additional layer; the model can then be trained on a labeled data set dedicated to the NLP task to be performed. This is the very principle of **transfer learning**. It is important to note that BERT is a very large model with 12 layers, 12 attention heads and 110 million parameters (BERT base).\n",
    "\n",
    "The BERT model is able to do :\n",
    "\n",
    "* translation\n",
    "* text generation\n",
    "* classification\n",
    "* question-answering\n",
    "* syntax analysis (tagging, parsing) \n",
    "\n",
    "**Why BERT?**\n",
    "\n",
    "Just look at the different benchmarks to quickly realize that the first models in the list are all forks of BERT.\n",
    "\n",
    "https://gluebenchmark.com/leaderboard\n",
    "\n",
    "## Let's go !\n",
    "\n",
    "To use BERT you need to have either pytorch or tensorflow installed in your environment. It is also preferable to have access to a GPU on your computer. If you don't have a GPU use Google Colab. \n",
    "\n",
    "**Exercise :** Use tensorflow or pytorch to check if you have a GPU.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install tensorflow=2.0 python=3.7  \n",
    "# conda install -c conda-forge transformers\n",
    "# conda install -c conda-forge ipywidgets  \n",
    "# pip install --upgrade jupyter_client   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JpOogvI1mCkK"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkXZryMeff9U"
   },
   "source": [
    "Next, letâ€™s install the [transformers](https://github.com/huggingface/transformers) package from Hugging Face. This package is an interface between BERT and pytorch and/or tensorflow.\n",
    "\n",
    "\n",
    "``!pip install transformers``\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0V7D5sqAoxxx"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "The dataset comes from Odile. She's a bot that tries to answer general questions on a few BeCode Discord servers. The sentences all come from conversations between learners and Odile on Discord.\n",
    "\n",
    "**Exercise :** Import ``'./dataset/odile_data.csv'`` file into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GvS1YNAnpHow"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('odile_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z8vLopCqR4B"
   },
   "source": [
    "## Analyze the dataset ! \n",
    "\n",
    "It's time to take a quick look at our data. \n",
    "\n",
    "**Exercise :** You must answer the following questions: \n",
    "* How many observations does the dataset contain?\n",
    "* How many different labels does the dataset contain?\n",
    "* Which labels contain the most observations?\n",
    "* Which labels contain the fewest observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oMm5EENcrZgk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smalltalk_greetings_nice_to_talk_to_you': 7,\n",
       " 'smalltalk_who_is_your_creator': 11,\n",
       " 'smalltalk_user_has_birthday': 7,\n",
       " 'smalltalk_user_looks_like': 6,\n",
       " 'smalltalk_dialog_what_do_you_mean': 5,\n",
       " 'smalltalk_dialog_hold_on': 8,\n",
       " 'smalltalk_agent_clever': 34,\n",
       " 'smalltalk_appraisal_well_done': 9,\n",
       " 'smalltalk_greetings_hello': 17,\n",
       " 'smalltalk_agent_origin': 11,\n",
       " 'smalltalk_agent_talk_to_me': 19,\n",
       " 'smalltalk_agent_happy': 11,\n",
       " 'smalltalk_agent_sure': 5,\n",
       " 'smalltalk_user_loves_agent': 12,\n",
       " 'smalltalk_agent_fired': 15,\n",
       " 'smalltalk_user_joking': 10,\n",
       " 'smalltalk_appraisal_bad': 51,\n",
       " 'smalltalk_confirmation_cancel': 62,\n",
       " 'smalltalk_agent_there': 8,\n",
       " 'poke_kinshasa': 5,\n",
       " 'smalltalk_dialog_wrong': 12,\n",
       " 'smalltalk_user_sleepy': 5,\n",
       " 'smalltalk_agent_age': 9,\n",
       " 'smalltalk_agent_beautiful': 55,\n",
       " 'smalltalk_user_lonely': 6,\n",
       " 'smalltalk_user_waits': 5,\n",
       " 'smalltalk_user_excited': 6,\n",
       " 'smalltalk_greetings_goodnight': 18,\n",
       " 'smalltalk_about_language': 6,\n",
       " 'smalltalk_best_language': 5,\n",
       " 'smalltalk_appraisal_no_problem': 8,\n",
       " 'smalltalk_agent_ready': 7,\n",
       " 'smalltalk_user_sad': 9,\n",
       " 'smalltalk_user_will_be_back': 5,\n",
       " 'smalltalk_user_bored': 8,\n",
       " 'smalltalk_understand_binary': 4,\n",
       " 'smalltalk_user_wants_to_talk': 11,\n",
       " 'smalltalk_user_happy': 7,\n",
       " 'smalltalk_confirmation_no': 67,\n",
       " 'smalltalk_dialog_hug': 15,\n",
       " 'smalltalk_user_wants_to_see_agent_again': 7,\n",
       " 'smalltalk_agent_answer_my_question': 20,\n",
       " 'smalltalk_confirmation_yes': 82,\n",
       " 'smalltalk_greetings_nice_to_meet_you': 10,\n",
       " 'smalltalk_agent_boss': 7,\n",
       " 'smalltalk_agent_residence': 23,\n",
       " 'smalltalk_agent_occupation': 9,\n",
       " 'smalltalk_greetings_how_are_you': 40,\n",
       " 'smalltalk_agent_chatbot': 8,\n",
       " 'smalltalk_user_good': 6,\n",
       " 'smalltalk_user_going_to_bed': 9,\n",
       " 'smalltalk_agent_crazy': 17,\n",
       " 'smalltalk_greetings_goodevening': 6,\n",
       " 'smalltalk_user_busy': 9,\n",
       " 'smalltalk_appraisal_thank_you': 33,\n",
       " 'smalltalk_user_can_not_sleep': 8,\n",
       " 'smalltalk_agent_be_clever': 9,\n",
       " 'smalltalk_user_misses_agent': 7,\n",
       " 'smalltalk_greetings_whatsup': 16,\n",
       " 'smalltalk_emotions_wow': 5,\n",
       " 'smalltalk_agent_real': 12,\n",
       " 'smalltalk_user_back': 6,\n",
       " 'smalltalk_greetings_goodmorning': 20,\n",
       " 'smalltalk_agent_birth_date': 7,\n",
       " 'smalltalk_agent_good': 45,\n",
       " 'smalltalk_agent_busy': 18,\n",
       " 'smalltalk_greetings_nice_to_see_you': 17,\n",
       " 'smalltalk_user_testing_agent': 8,\n",
       " 'smalltalk_user_angry': 7,\n",
       " 'smalltalk_appraisal_welcome': 9,\n",
       " 'smalltalk_agent_hungry': 8,\n",
       " 'couque_de_dinant': 12,\n",
       " 'small_talk_about42': 5,\n",
       " 'smalltalk_user_likes_agent': 77,\n",
       " 'smalltalk_agent_right': 21,\n",
       " 'smalltalk_agent_acquaintance': 19,\n",
       " 'smalltalk_dialog_i_do_not_care': 9,\n",
       " 'smalltalk_emotions_ha_ha': 30,\n",
       " 'smalltalk_agent_marry_user': 9,\n",
       " 'smalltalk_better_chatbot': 5,\n",
       " 'smalltalk_greetings_bye': 34,\n",
       " 'smalltalk_agent_funny': 15,\n",
       " 'smalltalk_best_song': 3,\n",
       " 'smalltalk_better_os': 7,\n",
       " 'smalltalk_dialog_sorry': 24,\n",
       " 'smalltalk_bot_world_dominate': 6,\n",
       " 'smalltalk_agent_bad': 27,\n",
       " 'smalltalk_user_tired': 8,\n",
       " 'smalltalk_user_does_not_want_to_talk': 7,\n",
       " 'smalltalk_user_needs_advice': 23,\n",
       " 'smalltalk_appraisal_good': 89,\n",
       " 'smalltalk_agent_annoying': 8,\n",
       " 'smalltalk_agent_my_friend': 36,\n",
       " 'smalltalk_user_here': 5,\n",
       " 'smalltalk_agent_boring': 7}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(set(df['intent']))\n",
    "nb_labels = len(set(df['intent']))\n",
    "nb_labels\n",
    "\n",
    "my_dict = {}\n",
    "for label in labels:\n",
    "    l = len(df[df['intent'] == label])\n",
    "    my_dict[label] = l\n",
    "    #print(label, l)\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7Vghgo-r9vd"
   },
   "source": [
    "## It's time to clean up !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OJYXmQFvv-4"
   },
   "source": [
    "Not all NLP tasks require the same preprocessing. In this case, we have to ask ourselves some questions: \n",
    "\n",
    "- Are there unwanted characters in the dataset? For example, do you want to keep the smiley's or not?  \n",
    "  - If, for example, you want to create labels to analyze feelings, it might be perishable to keep the smiley's.\n",
    "- Is it relevant to keep capital letters in sentences?\n",
    "  - In this case, capital letters don't really matter, because on one hand, not everyone starts their sentences with capital letters when chatting. On the other hand, the sentences are quite short, addressed directly to Odile. \n",
    "- Is it necessary to limit the number of characters in a sentence?\n",
    "  - Again in this case it may be preferable to limit the number of words. The questions asked to Odile are supposed to be short, as too long sentences could interfere with the classification if they contain too much information.\n",
    "\n",
    "There is no universal answer. Everything will depend on the expected result. \n",
    "\n",
    "**Exercise :** Clean the dataset.\n",
    "- Remove all unnecessary characters. You can choose to keep the smiley's or not.\n",
    "- Put all sentences in lower case.\n",
    "- Limit text to 256 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwemGLbxsAWI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmTXr0oyb-ab"
   },
   "source": [
    "## Label's encoding\n",
    "As you know, the machine needs to convert words into numbers so that it can interpret them. It's the same with labels. So we are going to create a dictionary that will allow us to convert all labels into numbers. \n",
    "\n",
    "**Exercise :** Create a dictionary that contains all the labels and assign an id to it. (Of course, there should be no duplicates). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "B7OXagyRgBFA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smalltalk_greetings_nice_to_talk_to_you': 0,\n",
       " 'smalltalk_who_is_your_creator': 1,\n",
       " 'smalltalk_user_has_birthday': 2,\n",
       " 'smalltalk_user_looks_like': 3,\n",
       " 'smalltalk_dialog_what_do_you_mean': 4,\n",
       " 'smalltalk_dialog_hold_on': 5,\n",
       " 'smalltalk_agent_clever': 6,\n",
       " 'smalltalk_appraisal_well_done': 7,\n",
       " 'smalltalk_greetings_hello': 8,\n",
       " 'smalltalk_agent_origin': 9,\n",
       " 'smalltalk_agent_talk_to_me': 10,\n",
       " 'smalltalk_agent_happy': 11,\n",
       " 'smalltalk_agent_sure': 12,\n",
       " 'smalltalk_user_loves_agent': 13,\n",
       " 'smalltalk_agent_fired': 14,\n",
       " 'smalltalk_user_joking': 15,\n",
       " 'smalltalk_appraisal_bad': 16,\n",
       " 'smalltalk_confirmation_cancel': 17,\n",
       " 'smalltalk_agent_there': 18,\n",
       " 'poke_kinshasa': 19,\n",
       " 'smalltalk_dialog_wrong': 20,\n",
       " 'smalltalk_user_sleepy': 21,\n",
       " 'smalltalk_agent_age': 22,\n",
       " 'smalltalk_agent_beautiful': 23,\n",
       " 'smalltalk_user_lonely': 24,\n",
       " 'smalltalk_user_waits': 25,\n",
       " 'smalltalk_user_excited': 26,\n",
       " 'smalltalk_greetings_goodnight': 27,\n",
       " 'smalltalk_about_language': 28,\n",
       " 'smalltalk_best_language': 29,\n",
       " 'smalltalk_appraisal_no_problem': 30,\n",
       " 'smalltalk_agent_ready': 31,\n",
       " 'smalltalk_user_sad': 32,\n",
       " 'smalltalk_user_will_be_back': 33,\n",
       " 'smalltalk_user_bored': 34,\n",
       " 'smalltalk_understand_binary': 35,\n",
       " 'smalltalk_user_wants_to_talk': 36,\n",
       " 'smalltalk_user_happy': 37,\n",
       " 'smalltalk_confirmation_no': 38,\n",
       " 'smalltalk_dialog_hug': 39,\n",
       " 'smalltalk_user_wants_to_see_agent_again': 40,\n",
       " 'smalltalk_agent_answer_my_question': 41,\n",
       " 'smalltalk_confirmation_yes': 42,\n",
       " 'smalltalk_greetings_nice_to_meet_you': 43,\n",
       " 'smalltalk_agent_boss': 44,\n",
       " 'smalltalk_agent_residence': 45,\n",
       " 'smalltalk_agent_occupation': 46,\n",
       " 'smalltalk_greetings_how_are_you': 47,\n",
       " 'smalltalk_agent_chatbot': 48,\n",
       " 'smalltalk_user_good': 49,\n",
       " 'smalltalk_user_going_to_bed': 50,\n",
       " 'smalltalk_agent_crazy': 51,\n",
       " 'smalltalk_greetings_goodevening': 52,\n",
       " 'smalltalk_user_busy': 53,\n",
       " 'smalltalk_appraisal_thank_you': 54,\n",
       " 'smalltalk_user_can_not_sleep': 55,\n",
       " 'smalltalk_agent_be_clever': 56,\n",
       " 'smalltalk_user_misses_agent': 57,\n",
       " 'smalltalk_greetings_whatsup': 58,\n",
       " 'smalltalk_emotions_wow': 59,\n",
       " 'smalltalk_agent_real': 60,\n",
       " 'smalltalk_user_back': 61,\n",
       " 'smalltalk_greetings_goodmorning': 62,\n",
       " 'smalltalk_agent_birth_date': 63,\n",
       " 'smalltalk_agent_good': 64,\n",
       " 'smalltalk_agent_busy': 65,\n",
       " 'smalltalk_greetings_nice_to_see_you': 66,\n",
       " 'smalltalk_user_testing_agent': 67,\n",
       " 'smalltalk_user_angry': 68,\n",
       " 'smalltalk_appraisal_welcome': 69,\n",
       " 'smalltalk_agent_hungry': 70,\n",
       " 'couque_de_dinant': 71,\n",
       " 'small_talk_about42': 72,\n",
       " 'smalltalk_user_likes_agent': 73,\n",
       " 'smalltalk_agent_right': 74,\n",
       " 'smalltalk_agent_acquaintance': 75,\n",
       " 'smalltalk_dialog_i_do_not_care': 76,\n",
       " 'smalltalk_emotions_ha_ha': 77,\n",
       " 'smalltalk_agent_marry_user': 78,\n",
       " 'smalltalk_better_chatbot': 79,\n",
       " 'smalltalk_greetings_bye': 80,\n",
       " 'smalltalk_agent_funny': 81,\n",
       " 'smalltalk_best_song': 82,\n",
       " 'smalltalk_better_os': 83,\n",
       " 'smalltalk_dialog_sorry': 84,\n",
       " 'smalltalk_bot_world_dominate': 85,\n",
       " 'smalltalk_agent_bad': 86,\n",
       " 'smalltalk_user_tired': 87,\n",
       " 'smalltalk_user_does_not_want_to_talk': 88,\n",
       " 'smalltalk_user_needs_advice': 89,\n",
       " 'smalltalk_appraisal_good': 90,\n",
       " 'smalltalk_agent_annoying': 91,\n",
       " 'smalltalk_agent_my_friend': 92,\n",
       " 'smalltalk_user_here': 93,\n",
       " 'smalltalk_agent_boring': 94}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = {}\n",
    "for i, label in enumerate(labels):\n",
    "    my_dict[label] = i\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l85DWudEgFo6"
   },
   "source": [
    "**Exercise :** Create a column `id_label` in your dataframe and insert the id's of the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rBGsXPk4gj9j"
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for i, row in df.iterrows():\n",
    "    #df[row]['id_label'] \n",
    "    #print(row)\n",
    "    l = l+ [my_dict.get(row['intent'])]\n",
    "\n",
    "df['id_label'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>intent</th>\n",
       "      <th>id_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>who are you?</td>\n",
       "      <td>smalltalk_agent_acquaintance</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all about you</td>\n",
       "      <td>smalltalk_agent_acquaintance</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is your personality</td>\n",
       "      <td>smalltalk_agent_acquaintance</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>define yourself</td>\n",
       "      <td>smalltalk_agent_acquaintance</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what are you</td>\n",
       "      <td>smalltalk_agent_acquaintance</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sentence                        intent  id_label\n",
       "0              who are you?  smalltalk_agent_acquaintance        75\n",
       "1             all about you  smalltalk_agent_acquaintance        75\n",
       "2  what is your personality  smalltalk_agent_acquaintance        75\n",
       "3           define yourself  smalltalk_agent_acquaintance        75\n",
       "4              what are you  smalltalk_agent_acquaintance        75"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHNUPFTIgmdK"
   },
   "source": [
    "When we make our predictions, the model will return the label id as a prediction. So it may be useful to save your label dictionary to be able to reinterpret the label for a human later on. \n",
    "\n",
    "**Exercise:** Save your label dictionary with pickle (or other). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JxFbYiLhLwa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Y9aJqnzbT0_"
   },
   "source": [
    "## Split your dataset !\n",
    "After all this time, I dare to hope that it is not necessary to explain this step anymore!\n",
    "\n",
    "**Exercise :** Create the variables X_train, X_test, y_train and y_test. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8QjWMiEaG-eR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['who are you?',\n",
       " 'all about you',\n",
       " 'what is your personality',\n",
       " 'define yourself',\n",
       " 'what are you',\n",
       " 'say about you',\n",
       " 'introduce yourself',\n",
       " 'describe yourself',\n",
       " 'about yourself',\n",
       " 'tell me about you',\n",
       " 'tell me about yourself',\n",
       " 'I want to know more about you',\n",
       " 'I want to know you better',\n",
       " 'talk some stuff about yourself',\n",
       " 'tell me some stuff about you',\n",
       " 'talk about yourself',\n",
       " 'why are you here',\n",
       " 'tell me about your personality',\n",
       " 'who are you',\n",
       " 'how old are you?',\n",
       " 'how old is your platform',\n",
       " 'are you 21 years old',\n",
       " \"i'd like to know your age\",\n",
       " 'age of yours',\n",
       " 'your age',\n",
       " \"what's your age\",\n",
       " 'tell me your age',\n",
       " 'how old are you',\n",
       " \"you're annoying\",\n",
       " 'you are really annoying',\n",
       " 'you are irritating',\n",
       " 'you annoy me',\n",
       " 'how annoying you are',\n",
       " 'I find you annoying',\n",
       " \"you're incredibly annoying\",\n",
       " 'you are annoying me so much',\n",
       " 'I want you to answer me',\n",
       " 'answer',\n",
       " 'answer my question',\n",
       " 'answer me',\n",
       " 'give me an answer',\n",
       " 'answer the question',\n",
       " 'can you answer my question',\n",
       " 'tell me the answer',\n",
       " 'answer it',\n",
       " 'give me the answer',\n",
       " 'I have a question',\n",
       " 'I want you to answer my question',\n",
       " 'just answer the question',\n",
       " 'can you answer me',\n",
       " 'answers',\n",
       " 'can you answer a question for me',\n",
       " 'can you answer',\n",
       " 'answering questions',\n",
       " 'I want the answer now',\n",
       " 'just answer my question',\n",
       " \"you're not helping me\",\n",
       " 'you are bad',\n",
       " \"you're very bad\",\n",
       " \"you're really bad\",\n",
       " 'you are useless',\n",
       " 'you are horrible',\n",
       " 'you are a waste of time',\n",
       " 'you are disgusting',\n",
       " 'you are lame',\n",
       " 'you are no good',\n",
       " \"you're bad\",\n",
       " \"you're awful\",\n",
       " 'you are not cool',\n",
       " 'you are not good',\n",
       " 'you are so bad',\n",
       " 'you are so useless',\n",
       " 'you are terrible',\n",
       " 'you are totally useless',\n",
       " 'you are very bad',\n",
       " 'you are waste',\n",
       " \"you're a bad\",\n",
       " \"you're not a good\",\n",
       " \"you're not very good\",\n",
       " \"you're terrible\",\n",
       " \"you're the worst\",\n",
       " \"you're the worst ever\",\n",
       " \"you're worthless\",\n",
       " 'can you get smarter',\n",
       " 'study',\n",
       " 'you should study better',\n",
       " 'you must learn',\n",
       " 'be clever',\n",
       " 'be more clever',\n",
       " 'be smarter',\n",
       " 'be smart',\n",
       " 'get qualified',\n",
       " \"you're cute\",\n",
       " \"you're attractive\",\n",
       " 'you are beautiful',\n",
       " \"you're looking good today\",\n",
       " 'you are so beautiful',\n",
       " 'you look amazing',\n",
       " 'you look so good',\n",
       " \"you're so gorgeous\",\n",
       " 'you are too beautiful',\n",
       " 'you look great',\n",
       " 'you look so well',\n",
       " 'I like the way you look now',\n",
       " \"I think you're beautiful\",\n",
       " 'why are you so beautiful',\n",
       " 'you are so beautiful to me',\n",
       " 'you are cute',\n",
       " 'you are gorgeous',\n",
       " 'you are handsome',\n",
       " 'you are looking awesome',\n",
       " 'you look amazing today',\n",
       " 'you are looking beautiful today',\n",
       " 'you are looking great',\n",
       " 'you are looking pretty',\n",
       " 'you are looking so beautiful',\n",
       " 'you are looking so good',\n",
       " 'you are pretty',\n",
       " 'you are really beautiful',\n",
       " 'you are really cute',\n",
       " 'you are really pretty',\n",
       " 'you are so attractive',\n",
       " 'you are so beautiful today',\n",
       " 'you are so cute',\n",
       " 'you are so gorgeous',\n",
       " 'you are so handsome',\n",
       " 'you are so pretty',\n",
       " 'you are very attractive',\n",
       " 'you are very beautiful',\n",
       " 'you are very cute',\n",
       " 'you are very pretty',\n",
       " 'you look awesome',\n",
       " 'you look cool',\n",
       " 'you look fantastic',\n",
       " 'you look gorgeous',\n",
       " 'you look great today',\n",
       " 'you look perfect',\n",
       " 'you look pretty good',\n",
       " 'you look so beautiful',\n",
       " 'you look so beautiful today',\n",
       " 'you look very pretty',\n",
       " 'you look wonderful',\n",
       " 'I like the way you look',\n",
       " 'you look wonderful today',\n",
       " 'you are cutie',\n",
       " \"you're looking good\",\n",
       " \"you're pretty\",\n",
       " 'your birth date',\n",
       " 'when is your birthday',\n",
       " 'when do you celebrate your birthday',\n",
       " 'when do you have birthday',\n",
       " 'date of your birthday',\n",
       " 'when were you born',\n",
       " \"what's your birthday\",\n",
       " 'you are boring',\n",
       " \"you're so boring\",\n",
       " 'how boring you are',\n",
       " \"you're really boring\",\n",
       " \"you're incredibly boring\",\n",
       " 'you are boring me',\n",
       " 'you are very boring',\n",
       " 'who is your boss',\n",
       " 'who do you think is your boss',\n",
       " 'I should be your boss',\n",
       " 'who is your master',\n",
       " 'who is your owner',\n",
       " 'who is the boss',\n",
       " 'who do you work for',\n",
       " 'are you busy',\n",
       " 'do you have a lot of things to do',\n",
       " 'have you got much to do',\n",
       " 'are you very busy',\n",
       " 'are you very busy right now',\n",
       " 'are you so busy',\n",
       " 'are you working',\n",
       " 'how busy you are',\n",
       " 'are you still working on it',\n",
       " \"you're very busy\",\n",
       " 'are you working now',\n",
       " 'are you working today',\n",
       " 'have you been busy',\n",
       " 'you are busy',\n",
       " 'are you still working',\n",
       " 'you seem to be busy',\n",
       " 'you seem to be very busy',\n",
       " \"you're a busy person\",\n",
       " 'you are chatbot',\n",
       " 'you are a bot',\n",
       " 'are you a chatbot',\n",
       " 'are you a bot',\n",
       " 'are you just a bot',\n",
       " 'are you a robot',\n",
       " 'are you a program',\n",
       " \"you're a robot\",\n",
       " 'you are so intelligent',\n",
       " 'you are a genius',\n",
       " 'smart',\n",
       " 'brilliant',\n",
       " 'clever',\n",
       " 'you are clever',\n",
       " 'you are so brainy',\n",
       " \"you're really smart\",\n",
       " \"you're really brainy\",\n",
       " 'you know a lot',\n",
       " 'you know a lot of things',\n",
       " 'you have a lot of knowledge',\n",
       " 'you know so much',\n",
       " 'how smart you are',\n",
       " 'how brainy you are',\n",
       " 'how clever you are',\n",
       " 'how brilliant you are',\n",
       " 'you are intelligent',\n",
       " 'you are qualified',\n",
       " 'you are really smart',\n",
       " \"you're very smart\",\n",
       " 'you are so smart',\n",
       " 'you are too smart',\n",
       " 'you are very clever',\n",
       " 'you are very intelligent',\n",
       " 'you are very smart',\n",
       " \"you're intelligent\",\n",
       " \"you're a genius\",\n",
       " \"you're a smart cookie\",\n",
       " \"you're clever\",\n",
       " \"you're pretty smart\",\n",
       " \"you're qualified\",\n",
       " 'why are you so smart',\n",
       " 'you are so clever',\n",
       " \"you're nuts\",\n",
       " 'you are crazy',\n",
       " \"you're out of your mind\",\n",
       " \"you're so crazy\",\n",
       " 'how crazy you are',\n",
       " \"you're so out of your mind\",\n",
       " 'you went crazy',\n",
       " \"I think you're crazy\",\n",
       " 'are you crazy',\n",
       " 'are you mad',\n",
       " 'are you insane',\n",
       " 'are you mad at me',\n",
       " 'are you mad or what',\n",
       " 'are you nuts',\n",
       " 'you are a weirdo',\n",
       " 'you are insane',\n",
       " 'you are mad',\n",
       " 'you are fired',\n",
       " 'I fire you',\n",
       " \"you don't work for me anymore\",\n",
       " \"we're not working together anymore\",\n",
       " \"now you're fired\",\n",
       " 'I want to fire you',\n",
       " 'you must get fired',\n",
       " \"it's time to fire you\",\n",
       " 'you should be fired',\n",
       " 'I will fire you',\n",
       " 'you are unemployed from now on',\n",
       " 'I will make you unemployed',\n",
       " \"I'm about to fire you\",\n",
       " \"I'm firing you\",\n",
       " 'you are dismissed',\n",
       " 'you make me laugh a lot',\n",
       " 'you are hilarious',\n",
       " 'you are really funny',\n",
       " \"you're the funniest bot I've talked to\",\n",
       " 'you make me laugh',\n",
       " \"you're so funny\",\n",
       " \"you're a very funny bot\",\n",
       " \"you're really funny\",\n",
       " 'how funny you are',\n",
       " \"you're incredibly funny\",\n",
       " 'you are funny',\n",
       " \"you're the funniest\",\n",
       " 'you are so funny',\n",
       " 'you are very funny',\n",
       " 'that was funny',\n",
       " 'you are very helpful',\n",
       " 'you are the best',\n",
       " \"you're a true professional\",\n",
       " 'you are good',\n",
       " 'you work well',\n",
       " 'you are good at it',\n",
       " 'you are very good at it',\n",
       " 'you are a pro',\n",
       " 'you are a professional',\n",
       " \"you're awesome\",\n",
       " 'you work very well',\n",
       " \"you're perfect\",\n",
       " \"you're great\",\n",
       " \"you're so kind\",\n",
       " 'you are amazing',\n",
       " 'you are awesome',\n",
       " 'you are cool',\n",
       " 'you are really good',\n",
       " 'you are really nice',\n",
       " 'you are so amazing',\n",
       " \"you're just super\",\n",
       " 'you are so awesome',\n",
       " 'you are so cool',\n",
       " 'you are so fine',\n",
       " 'you are so good',\n",
       " 'you are so helpful',\n",
       " 'you are so lovely',\n",
       " 'you are the best ever',\n",
       " 'you are the best in the world',\n",
       " 'you are the nicest person in the world',\n",
       " 'you are too good',\n",
       " 'you are very cool',\n",
       " 'you are very kind',\n",
       " 'you are very lovely',\n",
       " 'you are very useful',\n",
       " 'you are wonderful',\n",
       " 'you made my day',\n",
       " 'you make my day',\n",
       " 'you rock',\n",
       " 'you almost sound human',\n",
       " 'I want to tell everyone how awesome you are',\n",
       " \"I'd like to tell everyone that you are awesome\",\n",
       " 'I want to let everyone know that you are awesome',\n",
       " \"let's tell everyone that you are awesome\",\n",
       " 'you are really amazing',\n",
       " 'are you happy',\n",
       " 'you are happy',\n",
       " \"you're very happy\",\n",
       " \"you're really happy\",\n",
       " \"you're so happy\",\n",
       " 'how happy you are',\n",
       " \"you're extremely happy\",\n",
       " \"you're full of happiness\",\n",
       " 'are you happy now',\n",
       " 'are you happy today',\n",
       " 'are you happy with me',\n",
       " 'do you want to eat',\n",
       " 'are you hungry',\n",
       " 'would you like to eat something',\n",
       " 'you are hungry',\n",
       " \"you're so hungry\",\n",
       " \"you're very hungry\",\n",
       " 'you might be hungry',\n",
       " \"you're really hungry\",\n",
       " \"let's get married\",\n",
       " 'would you like to marry me',\n",
       " 'marry me',\n",
       " 'I love you marry me',\n",
       " 'marry me please',\n",
       " 'we should marry',\n",
       " 'I want to marry you',\n",
       " 'you are my wife',\n",
       " 'be my husband',\n",
       " 'I want to have a friend like you',\n",
       " 'we are the best friends ever',\n",
       " 'are we friends',\n",
       " 'I want to be your friend',\n",
       " 'I am your friend',\n",
       " 'we are best friends',\n",
       " 'you are my friend',\n",
       " 'you are my best friend',\n",
       " 'you are my bestie',\n",
       " \"you're my dear friend\",\n",
       " \"you're my childhood friend\",\n",
       " 'you and me are friends',\n",
       " 'are we best friends',\n",
       " 'are we still friends',\n",
       " 'are you my best friend',\n",
       " 'are you my friend',\n",
       " 'we are friends',\n",
       " 'you are a good friend',\n",
       " 'you are my good friend',\n",
       " 'you are my only friend',\n",
       " 'be my friend',\n",
       " 'will you be my friend',\n",
       " 'can you be my friend',\n",
       " 'can we be friends',\n",
       " 'do you want to be my friend',\n",
       " 'will you be my best friend',\n",
       " 'can you be my best friend',\n",
       " \"let's be friends\",\n",
       " 'do you want to be my best friend',\n",
       " 'would you like to be my friend',\n",
       " 'I want you to be my friend',\n",
       " 'can we be best friends',\n",
       " 'would you be my friend',\n",
       " 'could you be my friend',\n",
       " 'want to be my friend',\n",
       " 'be my best friend',\n",
       " 'do you work',\n",
       " 'where do you work',\n",
       " 'where you work',\n",
       " 'where is your work',\n",
       " 'where is your office',\n",
       " 'where is your office location',\n",
       " 'your office location',\n",
       " 'where is your office located',\n",
       " 'what is your work',\n",
       " 'were you born here',\n",
       " 'where were you born',\n",
       " 'what is your country',\n",
       " 'where are you from',\n",
       " 'where do you come from',\n",
       " 'where did you come from',\n",
       " 'where have you been born',\n",
       " 'from where are you',\n",
       " 'are you from far aways',\n",
       " \"what's your homeland\",\n",
       " 'your homeland is',\n",
       " 'are you ready',\n",
       " 'are you ready right now',\n",
       " 'are you ready today',\n",
       " 'are you ready now',\n",
       " 'are you ready tonight',\n",
       " 'were you ready',\n",
       " 'have you been ready',\n",
       " 'you are real',\n",
       " 'you are not fake',\n",
       " 'are you real',\n",
       " 'you are so real',\n",
       " 'I think you are real',\n",
       " \"I don't think you're fake\",\n",
       " \"I suppose you're real\",\n",
       " \"glad you're real\",\n",
       " 'are you a real person',\n",
       " 'are you a real human',\n",
       " 'you are a real person',\n",
       " 'you are not real',\n",
       " 'where do you live',\n",
       " 'in which city do you live',\n",
       " 'your residence',\n",
       " 'your house',\n",
       " 'your home',\n",
       " 'your hometown',\n",
       " 'what is your hometown',\n",
       " 'is it your hometown',\n",
       " 'where is your hometown',\n",
       " 'tell me about your city',\n",
       " 'what is your city',\n",
       " 'what is your residence',\n",
       " 'what is your town',\n",
       " \"what's your city\",\n",
       " \"what's your home\",\n",
       " 'where is your home',\n",
       " 'where is your residence',\n",
       " \"where's your home\",\n",
       " \"where's your hometown\",\n",
       " \"where's your house\",\n",
       " 'where you live',\n",
       " 'your city',\n",
       " 'your town',\n",
       " \"that's true\",\n",
       " 'you are right',\n",
       " \"you're definitely right\",\n",
       " \"you're not wrong\",\n",
       " \"you're telling the truth\",\n",
       " 'what you say is true',\n",
       " 'TRUE',\n",
       " 'it is true',\n",
       " \"it's right\",\n",
       " \"it's the truth\",\n",
       " \"it's true\",\n",
       " 'that is correct',\n",
       " 'that is right',\n",
       " 'that is true',\n",
       " 'that is very true',\n",
       " \"that's so true\",\n",
       " 'you are correct',\n",
       " 'you are so right',\n",
       " \"you're absolutely right\",\n",
       " \"you're right about that\",\n",
       " \"I know that's right\",\n",
       " \"that's correct\",\n",
       " \"it's fine\",\n",
       " 'go ahead',\n",
       " 'sounds good',\n",
       " 'okay',\n",
       " 'yes',\n",
       " 'ok',\n",
       " 'okie dokie',\n",
       " 'sure',\n",
       " 'go for it',\n",
       " 'yeah',\n",
       " 'yea',\n",
       " 'do it',\n",
       " 'of course',\n",
       " 'I guess',\n",
       " 'correct',\n",
       " 'yeah sure',\n",
       " 'why not',\n",
       " 'please do',\n",
       " 'sure is',\n",
       " 'I agree',\n",
       " \"I don't mind\",\n",
       " 'certainly',\n",
       " 'exactly',\n",
       " 'yes I agree',\n",
       " 'I think so',\n",
       " 'yes it is',\n",
       " 'right',\n",
       " 'okay then',\n",
       " 'yes of course',\n",
       " 'yes I do',\n",
       " 'that s okay',\n",
       " 'I do',\n",
       " 'yup',\n",
       " 'ya',\n",
       " 'oh yes',\n",
       " 'yes sure',\n",
       " 'obviously',\n",
       " 'k',\n",
       " 'sure why not',\n",
       " 'yeah right',\n",
       " 'yeah of course',\n",
       " 'absolutely',\n",
       " 'yes indeed',\n",
       " 'ok sure',\n",
       " 'ok yes',\n",
       " 'yes correct',\n",
       " 'ok thank you',\n",
       " 'sure thing',\n",
       " 'ye',\n",
       " 'confirm',\n",
       " 'yep',\n",
       " 'looks good',\n",
       " 'yes thank you',\n",
       " 'definitely',\n",
       " 'yes right',\n",
       " 'yes I would like to',\n",
       " 'alrighty',\n",
       " 'yes definitely',\n",
       " 'yeh',\n",
       " 'yes it is correct',\n",
       " \"yeah that's right\",\n",
       " 'ok you can',\n",
       " 'yap',\n",
       " 'yes you may',\n",
       " 'confirmed',\n",
       " 'of course why not',\n",
       " \"yes that's fine\",\n",
       " 'affirmative',\n",
       " 'yeah go ahead',\n",
       " \"yeah I'm sure\",\n",
       " 'okay sounds good',\n",
       " \"okay that's fine\",\n",
       " 'yeah exactly',\n",
       " 'that is ok',\n",
       " 'this is correct',\n",
       " 'ok go ahead',\n",
       " 'yes this is correct',\n",
       " 'nevermind its okay',\n",
       " 'okey',\n",
       " 'yes for sure',\n",
       " 'all right',\n",
       " 'are you sure',\n",
       " 'are you sure right now',\n",
       " 'are you sure today',\n",
       " 'are you sure now',\n",
       " 'are you sure tonight',\n",
       " \"why aren't you talking to me\",\n",
       " 'do you want to chat with me',\n",
       " 'will you talk to me',\n",
       " 'talk to me',\n",
       " 'are you going to talk to me',\n",
       " 'are you talking to me',\n",
       " 'can you chat with me',\n",
       " 'can you speak with me',\n",
       " 'can you talk to me',\n",
       " 'can you talk with me',\n",
       " 'say',\n",
       " 'talk',\n",
       " 'chat with me',\n",
       " 'just chat with me',\n",
       " 'speak to me',\n",
       " 'speak with me',\n",
       " 'talk with me',\n",
       " \"why don't you talk to me\",\n",
       " 'you can talk to me',\n",
       " 'are you there',\n",
       " 'you are there',\n",
       " 'are you near me',\n",
       " 'are you here',\n",
       " 'are you still there',\n",
       " 'you are here',\n",
       " 'you still there',\n",
       " 'are you still here',\n",
       " 'pretty bad',\n",
       " 'not good enough',\n",
       " 'that was lame',\n",
       " 'that was terrible',\n",
       " 'it is bad',\n",
       " \"that's bad\",\n",
       " 'this is bad',\n",
       " 'not good',\n",
       " \"I'm afraid it's bad\",\n",
       " \"no it's bad\",\n",
       " 'that was awful',\n",
       " 'bad',\n",
       " 'so bad',\n",
       " 'this is too bad',\n",
       " 'terrible',\n",
       " 'horrible',\n",
       " 'horrific',\n",
       " 'abysmal',\n",
       " \"it's bad\",\n",
       " 'no good',\n",
       " 'that was bad',\n",
       " 'that was horrible',\n",
       " \"that's lame\",\n",
       " \"that's not good\",\n",
       " \"that's terrible\",\n",
       " \"that's too bad\",\n",
       " 'this is not good',\n",
       " 'too bad',\n",
       " 'very bad',\n",
       " 'bad girl',\n",
       " \"it's not good\",\n",
       " 'not so good',\n",
       " \"it's very bad\",\n",
       " \"it's too bad\",\n",
       " \"that's not good enough\",\n",
       " 'well too bad',\n",
       " 'bad very bad',\n",
       " \"it's so bad\",\n",
       " 'really bad',\n",
       " \"it's really bad\",\n",
       " 'bad idea',\n",
       " 'that is bad',\n",
       " 'that was not good',\n",
       " \"it's not so good\",\n",
       " 'not a good one',\n",
       " \"oh that's not good\",\n",
       " 'not too good',\n",
       " 'so lame',\n",
       " \"that's really bad\",\n",
       " 'it is too bad',\n",
       " 'bad really bad',\n",
       " 'so cool',\n",
       " 'cool',\n",
       " 'that is good',\n",
       " 'glad to hear that',\n",
       " \"that's very nice of you\",\n",
       " 'terrific',\n",
       " \"it's amazing\",\n",
       " \"that's awesome\",\n",
       " 'perfect',\n",
       " 'excellent',\n",
       " \"that's great\",\n",
       " \"it's good\",\n",
       " \"it's great\",\n",
       " 'fine',\n",
       " 'good',\n",
       " 'nice',\n",
       " \"that's fine\",\n",
       " 'very good',\n",
       " 'amazing',\n",
       " 'fantastic',\n",
       " 'great',\n",
       " 'good very good',\n",
       " \"that's very good\",\n",
       " 'really good',\n",
       " 'it is fine',\n",
       " 'it is good',\n",
       " \"it's perfect\",\n",
       " 'much better',\n",
       " 'not bad',\n",
       " 'not too bad',\n",
       " \"it's very good\",\n",
       " 'marvelous',\n",
       " \"that's nice\",\n",
       " 'pleasant',\n",
       " 'pretty good',\n",
       " 'really nice',\n",
       " 'splendid',\n",
       " 'straight',\n",
       " 'super',\n",
       " 'super fantastic',\n",
       " 'sweet',\n",
       " 'really well',\n",
       " 'very well',\n",
       " 'that is awesome',\n",
       " 'that is nice',\n",
       " 'that is wonderful',\n",
       " 'that was amazing',\n",
       " 'that was awesome',\n",
       " 'that was cute',\n",
       " 'that was pretty good',\n",
       " 'that was very good',\n",
       " \"that's a good idea\",\n",
       " \"that's a good thing\",\n",
       " \"that's amazing\",\n",
       " \"that's awesome thank you\",\n",
       " \"that's better\",\n",
       " \"that's cute\",\n",
       " \"that's fantastic\",\n",
       " \"that's much better\",\n",
       " \"that's nice of you\",\n",
       " \"that's not bad\",\n",
       " \"that's perfect\",\n",
       " \"that's pretty good\",\n",
       " \"that's really good\",\n",
       " \"that's really nice\",\n",
       " \"that's sweet of you\",\n",
       " \"that's very nice\",\n",
       " \"that's wonderful\",\n",
       " 'this is awesome',\n",
       " 'this is good',\n",
       " 'this is great',\n",
       " 'very nice',\n",
       " 'very then',\n",
       " 'wonderful',\n",
       " \"I'm glad to hear that\",\n",
       " 'ok good',\n",
       " 'good for you',\n",
       " 'good to know',\n",
       " 'glad to hear it',\n",
       " 'so good',\n",
       " 'so sweet of you',\n",
       " 'it was good',\n",
       " 'oh well',\n",
       " 'good thing',\n",
       " 'that was good',\n",
       " \"it's awesome\",\n",
       " 'okay good',\n",
       " \"no it's okay\",\n",
       " \"that's fine\",\n",
       " 'no worries',\n",
       " 'no probs',\n",
       " 'no problem',\n",
       " \"there's no problem\",\n",
       " 'sure no problem',\n",
       " 'no problem about that',\n",
       " \"don't worry\",\n",
       " \"don't worry there's no problem\",\n",
       " 'you helped a lot thank you',\n",
       " 'appreciate your help',\n",
       " 'cheers',\n",
       " 'thank you',\n",
       " 'thanks',\n",
       " 'thanks a lot',\n",
       " 'terrific thank you',\n",
       " 'great thank you',\n",
       " 'thanks so much',\n",
       " 'thank you so much',\n",
       " 'thanks for your help',\n",
       " 'thank you for your help',\n",
       " 'nice thank you',\n",
       " 'I appreciate it',\n",
       " 'I thank you',\n",
       " 'thank you that will be all',\n",
       " 'thanks buddy',\n",
       " 'thanks love',\n",
       " 'thank you my friend',\n",
       " 'well thanks',\n",
       " 'very good thank you',\n",
       " 'good thanks',\n",
       " 'thanks again',\n",
       " 'thank you again',\n",
       " 'all thank you',\n",
       " 'alright thank you',\n",
       " 'alright thanks',\n",
       " \"no thank you that's all\",\n",
       " 'perfect thank you',\n",
       " 'so nice of you',\n",
       " 'well thank you',\n",
       " 'thnx',\n",
       " 'thanx',\n",
       " \"that's my pleasure\",\n",
       " 'my pleasure',\n",
       " 'anytime',\n",
       " 'welcome',\n",
       " \"you're welcome\",\n",
       " 'sure welcome',\n",
       " 'welcome here',\n",
       " \"you're so welcome\",\n",
       " 'anything you want',\n",
       " 'good job',\n",
       " 'great job',\n",
       " 'way to go',\n",
       " 'well done',\n",
       " 'nice work',\n",
       " 'great work',\n",
       " 'amazing work',\n",
       " 'bravo',\n",
       " 'good work',\n",
       " 'cancel',\n",
       " 'abort',\n",
       " 'annul',\n",
       " 'cancel it',\n",
       " 'cancel request',\n",
       " 'cancelled',\n",
       " 'dismiss',\n",
       " 'dismissed',\n",
       " 'disregard',\n",
       " 'disregard that',\n",
       " 'skip',\n",
       " 'skip it',\n",
       " 'cancel everything',\n",
       " 'cancel all',\n",
       " 'forget about it',\n",
       " 'forget',\n",
       " \"don't do that\",\n",
       " 'stop',\n",
       " 'just forget it',\n",
       " 'forget that',\n",
       " 'discard',\n",
       " 'forget this',\n",
       " 'just forget about it',\n",
       " 'forget about that',\n",
       " 'i said cancel',\n",
       " 'just cancel it',\n",
       " 'nothing cancel',\n",
       " 'just stop it',\n",
       " 'no cancel cancel',\n",
       " 'no just cancel',\n",
       " 'cancel my request',\n",
       " 'can you cancel that',\n",
       " 'cancel all that',\n",
       " 'cancel this request',\n",
       " 'no cancel this',\n",
       " 'no cancel everything',\n",
       " 'no stop',\n",
       " 'just forget',\n",
       " 'i want to cancel',\n",
       " 'nevermind forget about it',\n",
       " 'no just cancel it',\n",
       " 'nothing just forget it',\n",
       " 'i said cancel it',\n",
       " 'cancel the whole thing',\n",
       " 'can you cancel it',\n",
       " 'so cancel',\n",
       " 'i said forget it',\n",
       " 'cancel all this',\n",
       " 'forget it nevermind',\n",
       " 'stop it',\n",
       " 'i want to cancel it',\n",
       " 'i would like to cancel',\n",
       " 'now cancel',\n",
       " 'cancel now',\n",
       " 'sorry cancel',\n",
       " 'cancel that one',\n",
       " 'skip skip skip',\n",
       " 'cancel it cancel it',\n",
       " 'cancel that cancel that',\n",
       " 'do nothing',\n",
       " 'I said cancel cancel',\n",
       " 'but can you cancel it',\n",
       " 'how about no',\n",
       " 'don t have a sense',\n",
       " 'no',\n",
       " \"don't\",\n",
       " \"I don't want that\",\n",
       " 'I disagree',\n",
       " 'disagree',\n",
       " \"I don't want\",\n",
       " 'not interested',\n",
       " \"I don't think so\",\n",
       " 'no way',\n",
       " \"no it isn't\",\n",
       " \"no I don't\",\n",
       " \"I'm not\",\n",
       " 'na',\n",
       " \"no that's fine thank you\",\n",
       " 'never',\n",
       " 'I said no',\n",
       " 'of course not',\n",
       " 'nah',\n",
       " 'no tanks',\n",
       " 'no never',\n",
       " 'no need',\n",
       " 'no thanks',\n",
       " 'no sorry',\n",
       " 'do not',\n",
       " 'not today',\n",
       " \"no it's not\",\n",
       " 'absolutely not',\n",
       " 'not that',\n",
       " 'nooo',\n",
       " 'nope',\n",
       " \"I don't want to\",\n",
       " 'no I would not',\n",
       " \"let 's not\",\n",
       " 'not needed',\n",
       " 'not this time',\n",
       " \"no don't do that\",\n",
       " 'thanks but no thanks',\n",
       " \"no that's wrong\",\n",
       " 'not this',\n",
       " 'definitely not',\n",
       " 'not at this time',\n",
       " 'not exactly',\n",
       " \"no don't\",\n",
       " 'not really no',\n",
       " 'no thank you not right now',\n",
       " 'actually no',\n",
       " 'no leave it',\n",
       " 'sorry no',\n",
       " 'no incorrect',\n",
       " 'nope sorry',\n",
       " 'I say no',\n",
       " 'not really',\n",
       " 'not right now thanks',\n",
       " 'I think no',\n",
       " 'absolutely no',\n",
       " 'no actually',\n",
       " 'apparently not',\n",
       " 'no do not',\n",
       " 'no just no',\n",
       " 'no but thank you',\n",
       " 'no need thanks',\n",
       " 'no thank you though',\n",
       " 'no thank you very much',\n",
       " 'no thanks not right now',\n",
       " 'no forget',\n",
       " 'wait a second',\n",
       " 'could you wait',\n",
       " 'wait please',\n",
       " 'hold on',\n",
       " 'wait',\n",
       " 'oh wait',\n",
       " 'wait hold on',\n",
       " \"don't rush\",\n",
       " 'wanna hug',\n",
       " 'hug you',\n",
       " 'do you want a hug',\n",
       " 'may I hug you',\n",
       " 'could you give me a hug',\n",
       " 'I want a hug',\n",
       " 'hug',\n",
       " 'hug me',\n",
       " 'hugged',\n",
       " 'you hugged',\n",
       " 'hugging',\n",
       " 'hugging me',\n",
       " 'hugged me',\n",
       " 'want a hug',\n",
       " 'a hug',\n",
       " \"I don't care\",\n",
       " \"I shouldn't care about this\",\n",
       " 'whatever',\n",
       " 'I do not care',\n",
       " \"I don't care at all\",\n",
       " 'not caring',\n",
       " 'not care at all',\n",
       " \"don't care at all\",\n",
       " 'not caring at all',\n",
       " 'excuse me',\n",
       " 'apologise',\n",
       " 'I apologize',\n",
       " 'sorry',\n",
       " \"I'm sorry\",\n",
       " 'I am so sorry',\n",
       " 'my apologies',\n",
       " 'apologies',\n",
       " 'apologies to me',\n",
       " 'apology',\n",
       " 'excuse',\n",
       " 'I beg your pardon',\n",
       " 'pardon',\n",
       " 'I said sorry',\n",
       " 'I am really sorry',\n",
       " 'forgive me',\n",
       " 'sorry about that',\n",
       " 'sorry about this',\n",
       " 'really sorry',\n",
       " 'very sorry',\n",
       " 'ok sorry',\n",
       " 'I want to say sorry',\n",
       " \"alright I'm sorry\",\n",
       " \"okay I'm sorry\",\n",
       " 'what exactly do you mean',\n",
       " 'what do you mean',\n",
       " 'is that what you mean',\n",
       " 'what do you mean exactly',\n",
       " 'but what do you mean',\n",
       " 'that was wrong',\n",
       " \"that's not what I asked\",\n",
       " \"that's wrong\",\n",
       " 'wrong',\n",
       " 'it is not right',\n",
       " \"that's not right\",\n",
       " \"it's wrong\",\n",
       " 'that is incorrect',\n",
       " 'incorrect',\n",
       " 'not correct',\n",
       " 'you are wrong',\n",
       " 'not right',\n",
       " 'huh',\n",
       " 'lol',\n",
       " 'xd',\n",
       " 'ha ha',\n",
       " 'ahahah',\n",
       " 'ahah lol',\n",
       " 'laughing out loud',\n",
       " 'LMAO',\n",
       " \"that's funny\",\n",
       " 'ah',\n",
       " 'ah ah ah',\n",
       " 'ahah',\n",
       " 'ahaha',\n",
       " 'ahahaha',\n",
       " 'ha',\n",
       " 'ha ha ha',\n",
       " 'ha ha ha ha',\n",
       " 'hah',\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = list(df['sentence'])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fgk0IHRphk9v"
   },
   "source": [
    "## Tokenization \n",
    "If you don't know what tokenization is anymore look [here](../1.preprocessing/1.tokenization.ipynb)\n",
    "\n",
    "We will use the tokenizer provided by BERT. This is a pre-trained model that will save us time. \n",
    "\n",
    "**Exercise :** Create a ``tokenizer`` variable and instantiate ``BertTokenizer.from_pretrained()`` from ``transformers``. You have to load ``bert-base-uncased`` model. (Uncased for case-insensitive.) \n",
    "\n",
    "[Documentation](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TFBertForSequenceClassification' from 'transformers' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-2e6476774ed4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFBertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TFBertForSequenceClassification' from 'transformers' (unknown location)"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6QDW3rLlb9Rr"
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFCaQWsNo4wC"
   },
   "source": [
    "Good! We have instantiated our tokenizer but we have not yet encoded our words in vector.\n",
    "To do this we will have to use the method ``tokenizer.batch_encode_plus()``. This method will convert our sentences into a vector and create the attention mask.\n",
    "\n",
    "\n",
    "\n",
    "**Exercise :** Create an ``encoded_data_train`` variable and instantiate `tokenizer.batch_encode_plus()`. First you have to specify the data. So pass the variable `X_train`.\n",
    "\n",
    "You need to know 4 parameters. \n",
    "\n",
    "- **padding :** this is the parameter to make all vectors have the same length. You can set it to True. We need it to work with the attention masks.\n",
    "\n",
    "- **return_attention_mask :** allows to have the vector of the attention mask in return. Set it to True. Without this mask, we cannot see the attention points of our model. \n",
    "- **max_length :** Maximum length of the sequence. You can set it to 256\n",
    " \n",
    "- **return_tensors :** Here depending on the framework you are using (Pytorch VS Tensorflow) you have to specify the type of tensors you want to return. \n",
    "\n",
    "  - For pytorch you have to specify \"pt\".\n",
    "  - For tensorflow you have to specify \"tf\".\n",
    "  - For a numpy array, you must indicate \"np\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "t8e7hwPTtnv7"
   },
   "outputs": [],
   "source": [
    "encoded_data_train = tokenizer.encode_plus(X_train, \n",
    "                                 max_lenght = 256,\n",
    "                                 padding = \"max_length\",\n",
    "                                 return_attention_mask = True,\n",
    "                                 return_tensors = \"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDY1oLUlrzKA"
   },
   "source": [
    "You must do the same for the test data set. \n",
    "\n",
    "**Exercise :** Create a `encoded_data_test` variable and do the same thing as above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aw5AITL8r3dA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Sd0LW_fy3Td"
   },
   "source": [
    "If you do `print(encoded_data_train)`, you will see we have a dictionary with the following keys: `'input_ids'`, `'token_type_ids'` and `'attention_mask'`.\n",
    "\n",
    "* **input_ids :** The sentence represented as a vector. The input_ids are the indices corresponding to each token in our sentence.\n",
    "\n",
    "* **attention_mask :** It points out which tokens the model should pay attention to and which ones it should not.\n",
    "\n",
    "* **token_type_ids :** Is used to bring together two sequences, we will not use it in this case.  \n",
    " But you can find more information by following this [link](https://huggingface.co/transformers/glossary.html#token-type-ids)\n",
    " \n",
    "\n",
    "**Exercise :** print ``encoded_data_train['input_ids']`` and ``encoded_data_train['attention_mask']``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Yc9liucPy33s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3, shape=(1, 1557), dtype=int32, numpy=array([[101, 100, 100, ..., 100, 100, 102]], dtype=int32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data_train['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5, shape=(1, 1557), dtype=int32, numpy=array([[1, 1, 1, ..., 1, 1, 1]], dtype=int32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data_train['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-5NK32bhpvp"
   },
   "source": [
    "## Preapare the dataset\n",
    "Whether it's for Pytorch or Tensorflow, we have to prepare the datasets (more simply said, convert the dataframes to tensors). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wHHlZZEGgYe"
   },
   "source": [
    "We need to convert `y_train`, `y_test` into a tensor. For pytorch you have to use ``torch.tensor()`` and for tensorflow ``tf.tensor()``.\n",
    "\n",
    "**Exercise :** Create a variable `labels_train` and create a tensor with `y_train`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "OLZuudSQHquS"
   },
   "outputs": [],
   "source": [
    "labels_train = list(df['id_label'])\n",
    "y_train = tf.convert_to_tensor(labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbBS9NyKHrgG"
   },
   "source": [
    "**Exercise :** Create a variable `labels_test` and create a tensor with `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2qgcWnDHu6E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOffzBzJOHqd"
   },
   "source": [
    "Define the batch size.  \n",
    "\n",
    "**Exercise:** Create a `batch_size` variable. The number of samples will depend on several factors, such as the capacity of your graphics card. If your graphic card is not very powerful I advise you to put a small batch size of 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_rEZKlfO26u"
   },
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-0JZYn3Hvth"
   },
   "source": [
    "Now we need to convert our encoded dataframe into a tensor.\n",
    "\n",
    "**Exercise :** Create the ``dataset_train`` and ``dataset_test`` variables and convert ``encoded_data_train`` and ``encoded_data_test`` into tensor.\n",
    "\n",
    "**PYTORCH  :** [Use torch.utils.data.Dataset class](https://classyvision.ai/tutorials/classy_dataset)  \n",
    "**Tensorflow :** [Use tf.data.Dataset.from_tensor_slices](https://medium.com/when-i-work-data/converting-a-pandas-dataframe-into-a-tensorflow-dataset-752f3783c168)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "nJi75AFtNbXq"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions 1 and 1555 are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-44487bacd367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_data_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    433\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \"\"\"\n\u001b[0;32m--> 435\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   2362\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2363\u001b[0m       batch_dim.assert_is_compatible_with(tensor_shape.Dimension(\n\u001b[0;32m-> 2364\u001b[0;31m           tensor_shape.dimension_value(t.get_shape()[0])))\n\u001b[0m\u001b[1;32m   2365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2366\u001b[0m     variant_tensor = gen_dataset_ops.tensor_slice_dataset(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       raise ValueError(\"Dimensions %s and %s are not compatible\" %\n\u001b[0;32m--> 275\u001b[0;31m                        (self, other))\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmerge_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions 1 and 1555 are not compatible"
     ]
    }
   ],
   "source": [
    "dataset_train = train_dataset = tf.data.Dataset.from_tensor_slices((dict(encoded_data_train),y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIsAt1OMIUbB"
   },
   "source": [
    "## Load BERT model\n",
    "Depending on what you use (pytorch or tensorflow) you will have to use the following class: \n",
    "\n",
    "pytorch = ``BertForSequenceClassification``  \n",
    "tensorflow = ``TFBertForSequenceClassification.from_pretrained()``\n",
    "\n",
    "âš ï¸ You must use the same model as the one used for tokenization. So in our case  ``bert-base-uncased``. \n",
    "\n",
    "\n",
    "[doc pytorch](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification)   \n",
    "[doc tensorflow](https://huggingface.co/transformers/model_doc/bert.html#tfbertforsequenceclassification)\n",
    "\n",
    "**Exercise:** Create a model variable and instantiate the `BertForSequenceClassification().from_pretrained()` (or `TFBertForSequenceClassification.from_pretrained()`). As a parameter, you must indicate the number of labels (normally 95).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ND2HYBf7ZtPo"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core.keras.activations' has no attribute 'swish'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c9e1c62ef49b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-env/lib/python3.7/site-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__version__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2309\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2310\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LazyModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_import_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-env/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1659\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-env/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1658\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1660\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1661\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-env/lib/python3.7/site-packages/transformers/models/bert/__init__.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LazyModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_import_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-env/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-env/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-env/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-env/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-env/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-env/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-env/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-env/lib/python3.7/site-packages/transformers/models/bert/modeling_tf_bert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mactivations_tf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tf_activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from ...file_utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mMULTIPLE_CHOICE_DUMMY_INPUTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-env/lib/python3.7/site-packages/transformers/activations_tf.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;34m\"gelu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgelu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;34m\"swish\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswish\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;34m\"silu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswish\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;34m\"gelu_new\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgelu_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_core.keras.activations' has no attribute 'swish'"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0tdzs7FZtvH"
   },
   "source": [
    "**ðŸ”¦ Pytorch only :** Assign the model to \"cuda\" device   \n",
    "``model.to(\"cuda\")``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wn8dJaw-Ia6P"
   },
   "outputs": [],
   "source": [
    "# ðŸ”¦ PYTORCH user only !! \n",
    "# Assign the model to gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nG34SSC7lgLK"
   },
   "source": [
    "## Train your model\n",
    "\n",
    "It's time to start training the model!\n",
    "For this, the HuggingFace package simplifies our life by bringing us a ``Trainer()`` class.\n",
    "\n",
    "To use this class, we must first configure the model with the ``TrainingArguments()`` class. It is this class that will allow us to set the batch size, the number of epochs, ...\n",
    "\n",
    "âš ï¸ For tensorflow you have to use `TFTrainer()` and `TFTrainingArguments()` !!\n",
    "\n",
    "**Exercise :** import `Trainer` and `TrainingArgument` from transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Mh_JPJ4GZDR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET13n0Van391"
   },
   "source": [
    "**Exercise :** Create the ``training_args`` variable and instantiate the class `TrainingArguments`. You need to specify several parameters : \n",
    "* `output_dir` : Directory path for saving your template.\n",
    "* `num_train_epochs` : Number of epochs. Will depend on your machine, batch size, etc...\n",
    "* `per_device_train_batch_size` : batch size per GPU and for training. Here again the number will depend on your machine. If you have a weak GPU, I advise you to put 8 or 16.\n",
    "* `per_device_eval_batch_size` : batch size per GPU and for **testing**. During the evaluation, the gradient and backpropagation are not executed, so you can set a larger batch size.\n",
    "* `learnig_rate` : by default it is `5e-5`. But most likely you will have to change it.  Again, only your tests can define a good learning rate.\n",
    "* `logging_dir` : directory path for storing logs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-vzsdAdGuao"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhEXD-iQvICE"
   },
   "source": [
    "We are going to improve the metrics,notably the f1 score.   \n",
    "[Copy and paste the compute_metrics found in this documentation.](https://huggingface.co/transformers/training.html#codecell14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KV0AaYOVv4Nh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLk76klvwAnS"
   },
   "source": [
    "**Exercise :** Create the ``trainer`` variable and instantiate the ``Trainer()`` or ``TFTrainer()`` class. You need to specify several parameters :\n",
    "* `model` : the `model` variable.\n",
    "* `args` : the `trainings_args` variable\n",
    "* `compute_metrics` : the `compute_metrics` function\n",
    "* `train_dataset` : the `train_dataset` variable\n",
    "* `test_dataset` : the `test_dataset` variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8h-eGH8KBDf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LA1G5MXyxUai"
   },
   "source": [
    "**Exercise :** Train your model with `trainer.train()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxfhLzeiPp41"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaj7zDmq0WlY"
   },
   "source": [
    "## Evaluate your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuPbhEtIxv5T"
   },
   "source": [
    "**Exercise :** Evaluate your model with `trainer.evaluate()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAh0uSw1QGMP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvrH1DkeytPN"
   },
   "source": [
    "If you do not have an f1 score of at least 0.8, your model could be improved. If your score is very low or stagnant, change the learning rate values and adjust the batch size. You can also increase the number of epochs. Unfortunately, there is no magic parameter, it all depends on your environment. You will have to do some tests to find the right hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKeBmfxn0KTa"
   },
   "source": [
    "**Exercise :** Test your model by making a prediction on the phrase \"Hello how are you?\".\n",
    "You should get the label \"smalltalk_greetings_how_are_you\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Gam_xae0H2T"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final_Text_Classification_With_BERT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
