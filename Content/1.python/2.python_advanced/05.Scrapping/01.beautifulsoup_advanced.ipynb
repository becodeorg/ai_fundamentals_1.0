{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targeted information retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how to parse a webpage, which retrieves the information without distinction.\n",
    "\n",
    "But, in general, the purpose of scrapping is to automate the collection of targeted information on the web\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say I want to scrape the description of the latest movies released in theaters\n",
    "\n",
    "So I go to the allociné website and try to find the tags that will give me links to the specific pages of these movies to get their summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery of the url of the pages of films newly released in the theaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='http://www.allocine.fr/'\n",
    "r = requests.get(url)\n",
    "print(url, r.status_code)\n",
    "soup = BeautifulSoup(r.content,'lxml')\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On your web browser (Chrome, Firefox,...), you can use the \"inspect\" function (right click -> inspect) and drag your mouse to the areas of the page that interest you. At the same time I will see the html script move to the instructions of the html script in question. \n",
    "\n",
    "That's how you find the tags that you are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I notice that the relative link of the web page specific to the new movie on the poster is stored in these tags:\n",
    "\n",
    "```html\n",
    "<a class=\"meta-title meta-title-link\" href=\"/film/fichefilm_gen_cfilm=235582.html\" title=\"Le Grand Bain\">Le Grand Bain</a>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in soup.find_all('a'):\n",
    "    print (p.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the site is more difficult to \"extract\". Let's use much more specific parameters to the search function `find_all`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In addition to the tag a, which is easily identifiable, we notice some additional \n",
    "# information such as the value of the class variable of these identical tags.\n",
    "for elem in soup.find_all('a',attrs={\"class\" :\"meta-title meta-title-link\"}):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery of `href`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have noticed the presence of `href` links to the pages that interest us. Let's go get them back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in soup.find_all('a',attrs={\"class\" :\"meta-title meta-title-link\"}):\n",
    "    print(elem.get('href'))\n",
    "    # return a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you retrieve the titles for me via the search for \"title\" in the items of the previous list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by building the url that we will use to retrieve the summaries\n",
    "\n",
    "Start by putting the `href` values in a list of links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links=[]\n",
    "for elem in soup.find_all('a',attrs={\"class\" :\"meta-title meta-title-link\"}):\n",
    "    # I simply put all of thisin a list\n",
    "    links.append(elem.get('href'))\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The absolute url of the searched movie pages is built in this form: http://www.allocine.fr/film/fichefilm_gen_cfilm=243835.html\n",
    "\n",
    "It is therefore necessary to repeat the previous list and build the absolute urls for our search\n",
    "\n",
    "It's up to you to play.\n",
    "\n",
    "NB: Do not take the links for the shows(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_movie=['http://www.allocine.fr'+ elem for elem in links if 'film' in elem]\n",
    "links_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, on each page, the title and synopsis must be retrieved. Let's try for a movie, the first of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=links_movie[0]\n",
    "r = requests.get(url)\n",
    "print(url, r.status_code)\n",
    "soup = BeautifulSoup(r.content,'lxml')\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For title \n",
    "```html\n",
    "<div class=\"titlebar-title titlebar-title-lg\">Le Grand Bain</div>\n",
    "```\n",
    "For the synopsis\n",
    "\n",
    "```html\n",
    "<div class=\"content-txt \" itemprop=\"description\"\n",
    "\n",
    " \n",
    "              C’est dans les couloirs de leur piscine municipale que Bertrand, Marcus, Simon, Laurent, Thierry et les autres s’entraînent sous l’autorité toute relative de Delphine, ancienne gloire des bassins. Ensemble, ils se sentent libres et utiles. Ils vont mettre toute leur énergie dans une discipline jusque-là propriété de la gent féminine : la natation synchronisée. Alors, oui c’est une idée plutôt bizarre, mais ce défi leur permettra de trouver un sens à leur vie...\n",
    "    \n",
    "      </div>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in soup.find_all('div',attrs={\"class\" :\"titlebar-title titlebar-title-lg\"}):\n",
    "    # Just like that\n",
    "    print(elem.text)\n",
    "    \n",
    "for elem in soup.find_all('div',attrs={\"class\" :\"content-txt \"}):\n",
    "    # Just like that\n",
    "    print(elem.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Automate this script for the entire list\n",
    "\n",
    "2) Put the information in three lists (film_links, title, synopsis)\n",
    "\n",
    "3) Create a dataframe that includes these three informations in three associated columns\n",
    "\n",
    "4) Save this dataframe in a csv file\n",
    "\n",
    "And here's your first real scrap, you're real hackers now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from random import randint\n",
    "\n",
    "title=[]\n",
    "synopsis=[]\n",
    "\n",
    "for link in links_movie:\n",
    "    \n",
    "    url=link\n",
    "    # I slow down the frequency of requests to avoid being identified and therefore ban from the site\n",
    "    time.sleep(random.uniform(1.0, 2.0))\n",
    "    r = requests.get(url)\n",
    "    print(url, r.status_code)\n",
    "    soup = BeautifulSoup(r.content,'lxml')\n",
    "    \n",
    "    \n",
    "    for elem in soup.find_all('div',attrs={\"class\" :\"titlebar-title titlebar-title-lg\"}):\n",
    "        title.append(elem.text.strip())\n",
    "\n",
    "    for elem in soup.find_all('div',attrs={\"class\" :\"content-txt \"}):\n",
    "        synopsis.append(elem.text.strip())\n",
    "        \n",
    "# I check the length of the lists before creating the df\n",
    "len(title),len(synopsis),len(links_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Titre':titre})\n",
    "df['synopsis']=synopsis\n",
    "df['liens']=liens_film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./assets/allo_cine.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
