{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "\n",
    "## Installation of spaCy\n",
    "\n",
    "There are two tools widely used in the NLP world. These are two libraries called respectively spacy and nltk. The choice of spacy is based on its ease of implementation.\n",
    "> If you want to know the differences between spacy and nltk, consult this article. \n",
    "> https://medium.com/@akankshamalhotra24/introduction-to-libraries-of-nlp-in-python-nltk-vs-spacy-42d7b2f128f2\n",
    "\n",
    "\n",
    "***Installation via pip***\n",
    "\n",
    "````\n",
    "pip3 install -U spacy\n",
    "````\n",
    "\n",
    "***Installation via conda***\n",
    "````\n",
    "conda install -c conda-forge spacy\n",
    "````\n",
    "\n",
    "**Then you must install the language module !**\n",
    "\n",
    "````\n",
    "python3 -m spacy download en_core_web_sm\n",
    "````\n",
    "\n",
    "## Installation of scikit-learn\n",
    "\n",
    "scikitlearn is an important package for machine learning. In this example we will use it to train a preprogrammed model.\n",
    "> Installation guide:\n",
    "> https://scikit-learn.org/stable/install.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c']\n",
      "ee\n",
      "ok\n",
      "['d', 'e', 'f']\n",
      "['a', 'b', 'c', 'ee', 'e', 'e', 'd', 'e', 'f']\n"
     ]
    }
   ],
   "source": [
    "total = []\n",
    "for i in [['a','b','c'],'ee',['d','e','f']]:\n",
    "    print(i)\n",
    "    if type(i) == str:\n",
    "        total += [i]\n",
    "    total += i\n",
    "\n",
    "print(total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will start with a simple example to familiarize you with the basic concepts of NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"I hate school\": -1, \"I hate apples\":-1, \"I love you\":1, \"I love trees\":1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "The model we'll create is a word-based language model, which means each input unit is a single word (alternatively, some language models learn subword units like characters).\n",
    "\n",
    "###  Tokenization\n",
    "\n",
    "The first pre-processing step is to tokenize each of the stories into (lowercased) individual words. In this first example we ask you to write your own tokenization function. Later we will teach you how to do this with Spacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a `text_to_tokens` function because we will reuse this piece of code for the tests.\n",
    "\n",
    "Few rules:\n",
    "\n",
    "* Make sure to detach the punction from the words. \n",
    "    * `['ok', ',', 'I', 'will', 'do', 'it', '.']` -> OK\n",
    "    * `['ok,', 'I', 'will', 'do', 'it.']` -> NOT ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tokens(text):  \n",
    "            \n",
    "    # ADD YOUR CODE HERE...\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "# !! ======== DO NOT CHANGE  ======== !!\n",
    "# Just some code to check that your function is working.\n",
    "# If you get AssertionError, your funciton is not doing what is expected.\n",
    "test_string = \"ok, I will do it. it's not an issue! But are you sure?\"\n",
    "test_tokens = ['ok', ',', 'I', 'will', 'do', 'it',  '.', 'it', \"'\", 's', 'not', 'an', 'issue', '!', 'But', 'are', 'you', 'sure', '?']\n",
    "assert text_to_tokens(test_string) == test_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary indexing\n",
    "\n",
    "Now that you have done the tokenization, you need to convert those tokens to numbers because as we know, computers can only process numbers.\n",
    "\n",
    "To simplest approach could be to give a number to each **new** word.\n",
    "\n",
    "```python\n",
    "text = ['my', 'simple', 'sentence', '.',  'my', 'simple',  'text', '.']\n",
    "\n",
    "vocabulary = {\n",
    "  \"my\": 0,\n",
    "  \"simple\": 1,\n",
    "  \"sentence\": 2,\n",
    "  \".\": 3,\n",
    "  \"text\": 4,\n",
    "}\n",
    "```\n",
    "\n",
    "Let's try it then! Create a function `make_vocabulary()` that take a list of words as input and return a dict where the key is the word and the value is an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 0, 'simple': 1, 'sentence': 2, '.': 3, 'text': 4}\n"
     ]
    }
   ],
   "source": [
    "def make_vocabulary(tokens):\n",
    "    \"\"\"\n",
    "    This function will create a Dict our of a List of String to provide a unique number to each unique string.\n",
    "    \n",
    "    :param tokens: List of string containing a word or a punctuation mark.\n",
    "    :return: A Dict containing all the unique text's  string in key and an unique Int as value.\n",
    "    \"\"\"\n",
    "    # ADD YOUR CODE HERE....\n",
    "\n",
    "    return dic\n",
    "\n",
    "text = ['my', 'simple', 'sentence', '.',  'my', 'simple',  'text', '.']\n",
    "print(make_vocabulary(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "Now it's time for the last preprocessing step! We will wrap all our work.\n",
    "You will use your created functions to create an embedding system.\n",
    "\n",
    "The goal is to convert your tokens to numbers using your vocabulary list.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "text = \"my simple sentence.  my simple  text.\"\n",
    "\n",
    "vocabulary = {\n",
    "  \"my\": 0,\n",
    "  \"simple\": 1,\n",
    "  \"sentence\": 2,\n",
    "  \".\": 3,\n",
    "  \"text\": 4,\n",
    "}\n",
    "\n",
    "embedded = [0, 1, 2, 3, 0, 1, 4, 3]\n",
    "```\n",
    "\n",
    "To create `embbeded` we used the `vocabulary` on `text`. So the first word of the sentence is `my`, if I do  `vocabulary['my']` I  will  get 0 because I attributed this value in the `vocabulary`. Then I can simply loop over each token, get their value  in the vocabulary list and add it to the embbeded list.\n",
    "\n",
    "\n",
    "Embedd the sentence: `\"my simple sentence.  my simple  text.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text):\n",
    "    \"\"\"Embed a string and return a list of int.\"\"\"\n",
    "    \n",
    "    # ADD YOUR CODE HERE\n",
    "    \n",
    "    return embedded\n",
    "\n",
    "# !! ======== DO NOT CHANGE  ======== !!\n",
    "# Just some code to check that your function is working.\n",
    "# If you get AssertionError, your funciton is not doing what is expected.\n",
    "text = \"my simple sentence.  my simple  text.\"\n",
    "assert embed_text(text) == [0, 1, 2, 3, 0, 1, 4, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the steps on the above given dataset\n",
    "### Step 1: create vocabulary\n",
    "\n",
    "In the next step we want to train a model to predict whether a sentence is positive or negative. In our training dataset, we have a few labeled examples which we can use to train the model. -1 means negative, 1 means positive.\n",
    "\n",
    "First we generate the vocabularium for this dataset. Can you see whay the first steps are necessary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 0, 'I': 1, 'apples': 2, 'trees': 3, 'love': 4, 'school': 5, 'hate': 6}\n"
     ]
    }
   ],
   "source": [
    "# first we determine the tokens for all the sentences in the dataset\n",
    "all_tokens = [text_to_tokens(text) for text in data.keys()]\n",
    "\n",
    "# we want to make a list of all the DIFFERENT tokens, so they can not be shown twice.\n",
    "concat_list = list(set([j for i in all_tokens for j in i]))\n",
    "\n",
    "#Now we can construct a vocabulary from this list of words.\n",
    "vocabulary = make_vocabulary(concat_list)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well you're result will be something like this:\n",
    "\n",
    "```python\n",
    "{'I': 0, 'York': 1, 'hate': 2, 'school': 3, 'you': 4, 'New': 5, 'apples': 6, 'love': 7, 'trees': 8}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: embed sentences in dataset\n",
    "We want to construct a matrix (= a list of lists), where every list contains the embedding of 1 sentence.\n",
    "\n",
    "Please take your time to understand the steps in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 6, 5], [1, 6, 2], [1, 4, 0], [1, 4, 3]]\n"
     ]
    }
   ],
   "source": [
    "#We initialize an empty matrix\n",
    "matrix = []\n",
    "#We loop over the lists of tokens of the different sentences\n",
    "for tokens in all_tokens:\n",
    "    #We embed the tokens for each sentence\n",
    "    embedded = [vocabulary[token] for token in tokens]\n",
    "    #we add the embedding to the matrix\n",
    "    matrix.append(embedded)\n",
    "    \n",
    "print(matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well you're result will be something like this:\n",
    "\n",
    "```python\n",
    "[[0, 2, 3], [0, 2, 6], [0, 7, 4], [0, 7, 8], [0, 7, 5, 1], [0, 2, 3], [0, 2, 6], [0, 7, 4], [0, 7, 8], [0, 7, 5, 1]]\n",
    "```\n",
    "\n",
    "Please take some time to validate that the numbers indeed correspond to the words in the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: train a model\n",
    "Here is where some magic will happen. Don't despair if you don't fully understand what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#We make a target value that contains all of the labels -1 or 1\n",
    "y = [text for text in data.values()]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 6, 5], [1, 6, 2], [1, 4, 0], [1, 4, 3]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, 1, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We train a model giving it our embedded sentence (mat) and the known labels (y). From this it will try to learn \n",
    "#which words are important for determining positive or negative sentiment.\n",
    "clf = LogisticRegression(random_state=0).fit(matrix, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 2]\n",
      "The model predicts that the class is:  1\n"
     ]
    }
   ],
   "source": [
    "#We now want to use this trained model predict the sentiment of a new sentence, the model has never seen before\n",
    "test = \"I love apples\"\n",
    "embedded = [vocabulary[token] for token in text_to_tokens(test)]\n",
    "\n",
    "print(\"The model predicts that the class is: \", clf.predict([embedded])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model correctly sees the sentence as positive. Now an other example. It will give an error, can you identify why it gives an error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"I hate fish\"\n",
    "embedded = [vocabulary[token] for token in text_to_tokens(test)]\n",
    "\n",
    "print(clf.predict([embedded]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
