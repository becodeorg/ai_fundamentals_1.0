{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "To understand a sentence, us as humans, read each word. We analyze the meaning of it and then we connect words together. It's the same for a machine. So the first step to almost any NLP task will be to slice sentences up in words.\n",
    "\n",
    "![tokens](https://www.kdnuggets.com/wp-content/uploads/text-tokens-tokenization-manning.jpg)\n",
    "\n",
    "It seems simple said like that, but you also have to slice punctuation, composed words (but not all of them),...\n",
    "\n",
    "It's a time consuming task, that's why people invented a \"Tokenization\" function.\n",
    "\n",
    "Let's have a look at how [Spacy](https://spacy.io/) handles that.\n",
    "\n",
    "## Installation\n",
    "\n",
    "You will need to install Spacy, to do that I let you search on [their website](https://spacy.io/).\n",
    "You will also need to download their `en_core_web_sm`. To do that you can type:\n",
    "```shell\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "## Tokenize the text\n",
    "\n",
    "Now that you installed Spacy, let's take a look at their basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#. Tokenize this document with SpaCy:\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "\n",
    "# Store the tokens in doc\n",
    "\n",
    "doc = #TO COMPLETE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, our text is tokenized, now we can see a lot of interesting features. But first of all, let's see what our tokens look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should look something like this:\n",
    "```\n",
    "When\n",
    "Sebastian\n",
    "Thrun\n",
    "started\n",
    "working\n",
    "on\n",
    "self\n",
    "-\n",
    "driving\n",
    "cars\n",
    "at\n",
    "Google\n",
    "in\n",
    "2007\n",
    ",\n",
    "few\n",
    "people\n",
    "outside\n",
    "of\n",
    "the\n",
    "company\n",
    "took\n",
    "him\n",
    "seriously\n",
    ".\n",
    "“\n",
    "I\n",
    "can\n",
    "tell\n",
    "you\n",
    "very\n",
    "senior\n",
    "CEOs\n",
    "of\n",
    "major\n",
    "American\n",
    "car\n",
    "companies\n",
    "would\n",
    "shake\n",
    "my\n",
    "hand\n",
    "and\n",
    "turn\n",
    "away\n",
    "because\n",
    "I\n",
    "was\n",
    "n’t\n",
    "worth\n",
    "talking\n",
    "to\n",
    ",\n",
    "”\n",
    "said\n",
    "Thrun\n",
    ",\n",
    "in\n",
    "an\n",
    "interview\n",
    "with\n",
    "Recode\n",
    "earlier\n",
    "this\n",
    "week\n",
    ".\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the punctuation and `-` have been separated from the word they were appended to.\n",
    "\n",
    "Spacy also applies a lot of other preprocessing steps that we will see later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
