{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUu8O6h9b3Xr"
   },
   "source": [
    "# Text classification with BERT\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from **Transformers**) is a NLP model developed by Google in 2018. It is a model that is already pre-trained on a 2,5000M (+- 170 GB) words corpus from Wikipedia. \n",
    "\n",
    "![bert](https://www.advisa.fr/wp-content/uploads/2019/10/google-bert-algorithm.jpg)\n",
    "\n",
    "To accomplish a particular NLP task, the pre-trained BERT model is used as a base and refined by adding an additional layer; the model can then be trained on a labeled data set dedicated to the NLP task to be performed. This is the very principle of **transfer learning**. It is important to note that BERT is a very large model with 12 layers, 12 attention heads and 110 million parameters (BERT base).\n",
    "\n",
    "The BERT model is able to do :\n",
    "\n",
    "* translation\n",
    "* text generation\n",
    "* classification\n",
    "* question-answering\n",
    "* syntax analysis (tagging, parsing) \n",
    "\n",
    "**Why BERT?**\n",
    "\n",
    "Just look at the different benchmarks to quickly realize that the first models in the list are all forks of BERT.\n",
    "\n",
    "https://gluebenchmark.com/leaderboard\n",
    "\n",
    "## Let's go !\n",
    "\n",
    "To use BERT you need to have either pytorch or tensorflow installed in your environment. It is also preferable to have access to a GPU on your computer. If you don't have a GPU use Google Colab. \n",
    "\n",
    "**Exercise :** Use tensorflow or pytorch to check if you have a GPU.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JpOogvI1mCkK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkXZryMeff9U"
   },
   "source": [
    "Next, let’s install the [transformers](https://github.com/huggingface/transformers) package from Hugging Face. This package is an interface between BERT and pytorch and/or tensorflow.\n",
    "\n",
    "\n",
    "``!pip install transformers``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9XDitnGg8A8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0V7D5sqAoxxx"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "The dataset comes from Odile. She's a bot that tries to answer general questions on a few BeCode Discord servers. The sentences all come from conversations between learners and Odile on Discord.\n",
    "\n",
    "**Exercise :** Import ``'./dataset/odile_data.csv'`` file into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvS1YNAnpHow"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z8vLopCqR4B"
   },
   "source": [
    "## Analyze the dataset ! \n",
    "\n",
    "It's time to take a quick look at our data. \n",
    "\n",
    "**Exercise :** You must answer the following questions: \n",
    "* How many observations does the dataset contain?\n",
    "* How many different labels does the dataset contain?\n",
    "* Which labels contain the most observations?\n",
    "* Which labels contain the fewest observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMm5EENcrZgk"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7Vghgo-r9vd"
   },
   "source": [
    "## It's time to clean up !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OJYXmQFvv-4"
   },
   "source": [
    "Not all NLP tasks require the same preprocessing. In this case, we have to ask ourselves some questions: \n",
    "\n",
    "- Are there unwanted characters in the dataset? For example, do you want to keep the smiley's or not?  \n",
    "  - If, for example, you want to create labels to analyze feelings, it might be perishable to keep the smiley's.\n",
    "- Is it relevant to keep capital letters in sentences?\n",
    "  - In this case, capital letters don't really matter, because on one hand, not everyone starts their sentences with capital letters when chatting. On the other hand, the sentences are quite short, addressed directly to Odile. \n",
    "- Is it necessary to limit the number of characters in a sentence?\n",
    "  - Again in this case it may be preferable to limit the number of words. The questions asked to Odile are supposed to be short, as too long sentences could interfere with the classification if they contain too much information.\n",
    "\n",
    "There is no universal answer. Everything will depend on the expected result. \n",
    "\n",
    "**Exercise :** Clean the dataset.\n",
    "- Remove all unnecessary characters. You can choose to keep the smiley's or not.\n",
    "- Put all sentences in lower case.\n",
    "- Limit text to 256 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwemGLbxsAWI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmTXr0oyb-ab"
   },
   "source": [
    "## Label's encoding\n",
    "As you know, the machine needs to convert words into numbers so that it can interpret them. It's the same with labels. So we are going to create a dictionary that will allow us to convert all labels into numbers. \n",
    "\n",
    "**Exercise :** Create a dictionary that contains all the labels and assign an id to it. (Of course, there should be no duplicates). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7OXagyRgBFA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l85DWudEgFo6"
   },
   "source": [
    "**Exercise :** Create a column `id_label` in your dataframe and insert the id's of the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBGsXPk4gj9j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHNUPFTIgmdK"
   },
   "source": [
    "When we make our predictions, the model will return the label id as a prediction. So it may be useful to save your label dictionary to be able to reinterpret the label for a human later on. \n",
    "\n",
    "**Exercise:** Save your label dictionary with pickle (or other). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JxFbYiLhLwa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Y9aJqnzbT0_"
   },
   "source": [
    "## Split your dataset !\n",
    "After all this time, I dare to hope that it is not necessary to explain this step anymore!\n",
    "\n",
    "**Exercise :** Create the variables X_train, X_test, y_train and y_test. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8QjWMiEaG-eR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fgk0IHRphk9v"
   },
   "source": [
    "## Tokenization \n",
    "If you don't know what tokenization is anymore look [here](../1.preprocessing/1.tokenization.ipynb)\n",
    "\n",
    "We will use the tokenizer provided by BERT. This is a pre-trained model that will save us time. \n",
    "\n",
    "**Exercise :** Create a ``tokenizer`` variable and instantiate ``BertTokenizer.from_pretrained()`` from ``transformers``. You have to load ``bert-base-uncased`` model. (Uncased for case-insensitive.) \n",
    "\n",
    "[Documentation](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QDW3rLlb9Rr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFCaQWsNo4wC"
   },
   "source": [
    "Good! We have instantiated our tokenizer but we have not yet encoded our words in vector.\n",
    "To do this we will have to use the method ``tokenizer.batch_encode_plus()``. This method will convert our sentences into a vector and create the attention mask.\n",
    "\n",
    "\n",
    "\n",
    "**Exercise :** Create an ``encoded_data_train`` variable and instantiate `tokenizer.batch_encode_plus()`. First you have to specify the data. So pass the variable `X_train`.\n",
    "\n",
    "You need to know 4 parameters. \n",
    "\n",
    "- **padding :** this is the parameter to make all vectors have the same length. You can set it to True. We need it to work with the attention masks.\n",
    "\n",
    "- **return_attention_mask :** allows to have the vector of the attention mask in return. Set it to True. Without this mask, we cannot see the attention points of our model. \n",
    "- **max_length :** Maximum length of the sequence. You can set it to 256\n",
    " \n",
    "- **return_tensors :** Here depending on the framework you are using (Pytorch VS Tensorflow) you have to specify the type of tensors you want to return. \n",
    "\n",
    "  - For pytorch you have to specify \"pt\".\n",
    "  - For tensorflow you have to specify \"tf\".\n",
    "  - For a numpy array, you must indicate \"np\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8e7hwPTtnv7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDY1oLUlrzKA"
   },
   "source": [
    "You must do the same for the test data set. \n",
    "\n",
    "**Exercise :** Create a `encoded_data_test` variable and do the same thing as above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aw5AITL8r3dA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Sd0LW_fy3Td"
   },
   "source": [
    "If you do `print(encoded_data_train)`, you will see we have a dictionary with the following keys: `'input_ids'`, `'token_type_ids'` and `'attention_mask'`.\n",
    "\n",
    "* **input_ids :** The sentence represented as a vector. The input_ids are the indices corresponding to each token in our sentence.\n",
    "\n",
    "* **attention_mask :** It points out which tokens the model should pay attention to and which ones it should not.\n",
    "\n",
    "* **token_type_ids :** Is used to bring together two sequences, we will not use it in this case.  \n",
    " But you can find more information by following this [link](https://huggingface.co/transformers/glossary.html#token-type-ids)\n",
    " \n",
    "\n",
    "**Exercise :** print ``encoded_data_train['input_ids']`` and ``encoded_data_train['attention_mask']``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yc9liucPy33s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-5NK32bhpvp"
   },
   "source": [
    "## Preapare the dataset\n",
    "Whether it's for Pytorch or Tensorflow, we have to prepare the datasets (more simply said, convert the dataframes to tensors). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wHHlZZEGgYe"
   },
   "source": [
    "We need to convert `y_train`, `y_test` into a tensor. For pytorch you have to use ``torch.tensor()`` and for tensorflow ``tf.tensor()``.\n",
    "\n",
    "**Exercise :** Create a variable `labels_train` and create a tensor with `y_train`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLZuudSQHquS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbBS9NyKHrgG"
   },
   "source": [
    "**Exercise :** Create a variable `labels_test` and create a tensor with `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2qgcWnDHu6E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOffzBzJOHqd"
   },
   "source": [
    "Define the batch size.  \n",
    "\n",
    "**Exercise:** Create a `batch_size` variable. The number of samples will depend on several factors, such as the capacity of your graphics card. If your graphic card is not very powerful I advise you to put a small batch size of 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_rEZKlfO26u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-0JZYn3Hvth"
   },
   "source": [
    "Now we need to convert our encoded dataframe into a tensor.\n",
    "\n",
    "**Exercise :** Create the ``dataset_train`` and ``dataset_test`` variables and convert ``encoded_data_train`` and ``encoded_data_test`` into tensor.\n",
    "\n",
    "**PYTORCH  :** [Use torch.utils.data.Dataset class](https://classyvision.ai/tutorials/classy_dataset)  \n",
    "**Tensorflow :** [Use tf.data.Dataset.from_tensor_slices](https://medium.com/when-i-work-data/converting-a-pandas-dataframe-into-a-tensorflow-dataset-752f3783c168)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJi75AFtNbXq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIsAt1OMIUbB"
   },
   "source": [
    "## Load BERT model\n",
    "Depending on what you use (pytorch or tensorflow) you will have to use the following class: \n",
    "\n",
    "pytorch = ``BertForSequenceClassification``  \n",
    "tensorflow = ``TFBertForSequenceClassification.from_pretrained()``\n",
    "\n",
    "⚠️ You must use the same model as the one used for tokenization. So in our case  ``bert-base-uncased``. \n",
    "\n",
    "\n",
    "[doc pytorch](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification)   \n",
    "[doc tensorflow](https://huggingface.co/transformers/model_doc/bert.html#tfbertforsequenceclassification)\n",
    "\n",
    "**Exercise:** Create a model variable and instantiate the `BertForSequenceClassification().from_pretrained()` (or `TFBertForSequenceClassification.from_pretrained()`). As a parameter, you must indicate the number of labels (normally 95).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ND2HYBf7ZtPo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0tdzs7FZtvH"
   },
   "source": [
    "**🔦 Pytorch only :** Assign the model to \"cuda\" device   \n",
    "``model.to(\"cuda\")``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wn8dJaw-Ia6P"
   },
   "outputs": [],
   "source": [
    "# 🔦 PYTORCH user only !! \n",
    "# Assign the model to gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nG34SSC7lgLK"
   },
   "source": [
    "## Train your model\n",
    "\n",
    "It's time to start training the model!\n",
    "For this, the HuggingFace package simplifies our life by bringing us a ``Trainer()`` class.\n",
    "\n",
    "To use this class, we must first configure the model with the ``TrainingArguments()`` class. It is this class that will allow us to set the batch size, the number of epochs, ...\n",
    "\n",
    "⚠️ For tensorflow you have to use `TFTrainer()` and `TFTrainingArguments()` !!\n",
    "\n",
    "**Exercise :** import `Trainer` and `TrainingArgument` from transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Mh_JPJ4GZDR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET13n0Van391"
   },
   "source": [
    "**Exercise :** Create the ``training_args`` variable and instantiate the class `TrainingArguments`. You need to specify several parameters : \n",
    "* `output_dir` : Directory path for saving your template.\n",
    "* `num_train_epochs` : Number of epochs. Will depend on your machine, batch size, etc...\n",
    "* `per_device_train_batch_size` : batch size per GPU and for training. Here again the number will depend on your machine. If you have a weak GPU, I advise you to put 8 or 16.\n",
    "* `per_device_eval_batch_size` : batch size per GPU and for **testing**. During the evaluation, the gradient and backpropagation are not executed, so you can set a larger batch size.\n",
    "* `learnig_rate` : by default it is `5e-5`. But most likely you will have to change it.  Again, only your tests can define a good learning rate.\n",
    "* `logging_dir` : directory path for storing logs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-vzsdAdGuao"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhEXD-iQvICE"
   },
   "source": [
    "We are going to improve the metrics,notably the f1 score.   \n",
    "[Copy and paste the compute_metrics found in this documentation.](https://huggingface.co/transformers/training.html#codecell14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KV0AaYOVv4Nh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLk76klvwAnS"
   },
   "source": [
    "**Exercise :** Create the ``trainer`` variable and instantiate the ``Trainer()`` or ``TFTrainer()`` class. You need to specify several parameters :\n",
    "* `model` : the `model` variable.\n",
    "* `args` : the `trainings_args` variable\n",
    "* `compute_metrics` : the `compute_metrics` function\n",
    "* `train_dataset` : the `train_dataset` variable\n",
    "* `test_dataset` : the `test_dataset` variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8h-eGH8KBDf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LA1G5MXyxUai"
   },
   "source": [
    "**Exercise :** Train your model with `trainer.train()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxfhLzeiPp41"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaj7zDmq0WlY"
   },
   "source": [
    "## Evaluate your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuPbhEtIxv5T"
   },
   "source": [
    "**Exercise :** Evaluate your model with `trainer.evaluate()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAh0uSw1QGMP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvrH1DkeytPN"
   },
   "source": [
    "If you do not have an f1 score of at least 0.8, your model could be improved. If your score is very low or stagnant, change the learning rate values and adjust the batch size. You can also increase the number of epochs. Unfortunately, there is no magic parameter, it all depends on your environment. You will have to do some tests to find the right hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKeBmfxn0KTa"
   },
   "source": [
    "**Exercise :** Test your model by making a prediction on the phrase \"Hello how are you?\".\n",
    "You should get the label \"smalltalk_greetings_how_are_you\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Gam_xae0H2T"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final_Text_Classification_With_BERT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "env_last",
   "language": "python",
   "name": "env_last"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
