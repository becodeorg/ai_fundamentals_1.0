{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please find the documentation here:\n",
    "\n",
    "Dense layer: https://keras.io/api/layers/core_layers/dense/\n",
    "\n",
    "Sequential model: https://keras.io/guides/sequential_model/\n",
    "\n",
    "to_categorical: https://keras.io/api/utils/python_utils/#to_categorical-function\n",
    "\n",
    "activation: https://keras.io/api/layers/activations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "import numpy as np\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform your data into matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the classes \n",
    "\n",
    "First we want to define the classes into which we want to classify the data. In the folder 'bbc' you may find 1 folder per category. Each folder holds a list of txt files, all representing one text that belongs to that specific category.\n",
    "\n",
    "Can you find a way to read the different categories automatically from the 'bbc' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entertainment', 'business', 'sport', 'politics', 'tech']\n"
     ]
    }
   ],
   "source": [
    "path_origin = 'bbc/'\n",
    "\n",
    "categories = \n",
    "\n",
    "#Don't change this.\n",
    "print(categories)\n",
    "assert sorted(categories) == ['business', 'entertainment', 'politics', 'sport', 'tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read the data into x and y\n",
    "\n",
    "Construct two lists x and y. \n",
    "- x has to be a list of strings where 1 string is the data within 1 .txt file and we want to read all the subfolders within /bbc/.\n",
    "- y has the same length of x but holds the corresponding label that belongs to the text in x. So the text on position 50 in x has the label on position 50 in y.\n",
    "\n",
    "Tip: sometimes their may be errors when trying to read a file. You don't have to focus on solving this problem. Just use try: ... except: to skip the files that pose problems. Be sure to only add a label to y when you have actually managed to read a file and add it to X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbc/entertainment/\n",
      "bbc/business/\n",
      "bbc/sport/\n",
      "bbc/politics/\n",
      "bbc/tech/\n",
      "2224 2224\n"
     ]
    }
   ],
   "source": [
    "X = \n",
    "y = \n",
    "\n",
    "\n",
    "#Don't change this.            \n",
    "print(len(X), len(y))\n",
    "assert len(X) == len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split you data\n",
    "\n",
    "We need to make a seperate test and training dataset. We use the train set to train a model. Then we use the test set to check how the model performs on data it has not seen before.\n",
    "\n",
    "For this you can use the train_test_split() of sklearn. Please look up how it works. We want to have 25% of the data of X in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[386, 510, 510, 417, 401]\n",
      "[283, 372, 399, 318, 296]\n",
      "[103, 138, 111, 99, 105]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is done, we want to make sure all categories are well present in both datasets. Please run the cell below and check how much of each category we have within each dataset. Do you think all categories are enough represented?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You don't have to change this, you just have to run it.\n",
    "print([y.count(cat) for cat in categories])\n",
    "print([y_train.count(cat) for cat in categories])\n",
    "print([y_test.count(cat) for cat in categories])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TF-IDF\n",
    "\n",
    "As a next step we want to transform our data into a matrix, using TF-IDF. We will use the predefined function of sklearn TFIDFVectorizer. Please have a look at the different arguments you can give. Are their any interesting arguments that might be interesting to add? Please make sure you leave max_features= num_features.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1668, 10000) (556, 10000)\n"
     ]
    }
   ],
   "source": [
    "#Choose the number of features you want to learn, \n",
    "#this is equal to the number of words you want to take into account in your vocabulary.\n",
    "num_features = 10000\n",
    "#Number of categories. We will need this later.\n",
    "num_cat = len(categories)\n",
    "\n",
    "#Play with this\n",
    "tfidf = TfidfVectorizer(max_features= num_features, ...)\n",
    "\n",
    "#Don't change this, but understand what is happening.\n",
    "def tfidf_features(txt, flag):\n",
    "    if flag == \"train\":\n",
    "        x = tfidf.fit_transform(txt)\n",
    "    else:\n",
    "        x = tfidf.transform(txt)\n",
    "    return x \n",
    "\n",
    "\n",
    "X_trans_train = tfidf_features(X_train, flag=\"train\")\n",
    "X_trans_test = tfidf_features(X_test, flag=\"test\")\n",
    "\n",
    "print(X_trans_train.shape, X_trans_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transform y into a matrix\n",
    "\n",
    "Now the y vector is also made out of string, but remember, computers don't understand words. We have to attach a number to each class and perform a 1-hot encoding. See image below to understand 1-hot encoding or read more on the following link:\n",
    "\n",
    "https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/#:~:text=A%20one%20hot%20encoding%20is,is%20marked%20with%20a%201.\n",
    "\n",
    "<img src=\"assets/1hot.png\" width=1000 height=1000 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1668,) (556,)\n"
     ]
    }
   ],
   "source": [
    "#You don't have to change any code in this cell. Please understand what is happening.\n",
    "#First we transform the text label to a number\n",
    "lb = LabelEncoder()\n",
    "lb.fit(categories)\n",
    "y_trans_train = lb.transform(y_train)\n",
    "y_trans_test = lb.transform(y_test)\n",
    "\n",
    "#Then we transform this to a one hot encoded matrix\n",
    "y_hot_train=utils.to_categorical(y_trans_train, num_cat)\n",
    "y_hot_test=utils.to_categorical(y_trans_test, num_cat)\n",
    "\n",
    "print(y_trans_train.shape, y_trans_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define your model\n",
    "\n",
    "We are ready preparing our input and output model. Now we have to indicate how we want to build the model. Please fill in the gaps + add as much hidden layers as you see necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "#define input layer\n",
    "model.add(layers.Dense(..., input_shape=(num_features,)))\n",
    "model.add(layers.Activation(activations.relu))\n",
    "\n",
    "#Add hidden layers\n",
    "\n",
    "model.add(layers.Dense(num_cat))\n",
    "model.add(layers.Activation(activations.softmax))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the model, we will train it by feeding it with the input and output matrices we have prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1668/1668 [==============================] - 4s 2ms/step - loss: 1.3966 - accuracy: 0.5342\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "nb_epochs = 1\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_trans_train, y_hot_train, batch_size=batch_size, epochs=nb_epochs,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let the model make predictions based on\n",
    "1. the input data it has already seen before\n",
    "2. the new test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predclass = model.predict_classes(X_trans_train,batch_size=batch_size)\n",
    "y_test_predclass = model.predict_classes(X_trans_test,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance of the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Prediction\n",
    "\n",
    "print(\"nnDeep Neural Network - Train accuracy:\",round(accuracy_score( y_trans_train, y_train_predclass),3))\n",
    "\n",
    "print(\"nDeep Neural Network - Test accuracy:\",round(accuracy_score( y_trans_test,y_test_predclass),3))\n",
    "\n",
    "print(\"nDeep Neural Network - Train Classification Report\")\n",
    "\n",
    "print (classification_report(y_trans_train,y_train_predclass))\n",
    "\n",
    "print(\"nDeep Neural Network - Test Classification Report\")\n",
    "\n",
    "print (classification_report(y_trans_test,y_test_predclass))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
